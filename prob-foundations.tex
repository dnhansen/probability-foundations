% Document setup
\documentclass[article, a4paper, 11pt, oneside]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{babel}

% Document info
\newcommand\doctitle{Conceptual foundations of probability theory}
\newcommand\docauthor{Danny Nygård Hansen}

% Formatting and layout
\usepackage[autostyle]{csquotes}
\renewcommand{\mktextelp}{(\textellipsis\unkern)}
\usepackage[final]{microtype}
\usepackage{xcolor}
\frenchspacing
\usepackage{latex-sty/articlepagestyle}
\usepackage{latex-sty/articlesectionstyle}

% Fonts
\usepackage[largesmallcaps,partialup]{kpfonts}
\DeclareSymbolFontAlphabet{\mathrm}{operators} % https://tex.stackexchange.com/questions/40874/kpfonts-siunitx-and-math-alphabets
\linespread{1.06}
\let\mathfrak\undefined
\usepackage{eufrak}
\usepackage{inconsolata}
\usepackage{amssymb}

% Hyperlinks
\usepackage{hyperref}
\definecolor{linkcolor}{HTML}{4f4fa3}
\hypersetup{%
	pdftitle=\doctitle,
	pdfauthor=\docauthor,
	colorlinks,
	linkcolor=linkcolor,
	citecolor=linkcolor,
	urlcolor=linkcolor,
	bookmarksnumbered=true
}

% Equation numbering
\numberwithin{equation}{chapter}

% Footnotes
\footmarkstyle{\textsuperscript{#1}\hspace{0.25em}}

% Mathematics
\usepackage{latex-sty/basicmathcommands}
\usepackage{latex-sty/framedtheorems}
\usepackage{latex-sty/probabilitycommands}
\usepackage{tikz-cd}
\usetikzlibrary{babel}

% Lists
\usepackage{enumitem}
\setenumerate[0]{label=\normalfont(\arabic*)}

% Bibliography
\usepackage[backend=biber, style=authoryear, maxcitenames=2, useprefix]{biblatex}
\addbibresource{references.bib}

% Title
\title{\doctitle}
\author{\docauthor}


% Section style -- add to section style .sty?
\setsubsecheadstyle{\normalfont\itshape}


% Preimage -- to be added to mathcommands .sty
\newcommand{\preim}{^{-1}}


\newcommand{\calN}{\mathcal{N}}
\DeclarePairedDelimiter{\nhoodfilteraux}{(}{)}
% \newcommand{\nhoodfilter}[1]{\calN\nhoodfilteraux{#1}}
\newcommand{\nhoodfilter}[1]{\calN_{#1}}


\newcommand{\calU}{\mathcal{U}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calW}{\mathcal{W}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calJ}{\mathcal{J}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calL}{\mathcal{L}}

\newcommand{\borel}[1]{\calB(#1)}
\DeclareMathOperator{\supp}{supp}

\let\oldP\P
\renewcommand{\P}{\mathbb{P}}

% Add to basicmathcommands.sty
\DeclarePairedDelimiter{\card}{\lvert}{\rvert}
\renewcommand{\symdiff}{\mathbin{\triangle}}


\begin{document}

\maketitle

\chapter{Introduction}

In the usual measure-theoretical formulation of probability theory, the following result is a corollary of the Law of Large Numbers:

\begin{theorem}[The frequency interpretation of probability]
    Let $\rvar{X}, \rvar{X}_1, \rvar{X}_2, \ldots$ be i.i.d. real-valued random variables on a probability space $(\Omega, \calF, \P)$. For every $B \in \borel{\reals}$ we have
    %
    \begin{equation*}
        \P(\rvar{X} \in B)
            = \lim_{n\to\infty} \frac{
                \card{ \set{j \in \{1, \ldots, n\} }{ \rvar{X}_j(\omega) \in B } }
            }{
                n
            }
    \end{equation*}
    %
    for $\P$-almost all $\omega \in \Omega$.
\end{theorem}

\begin{proof}
    Thorbjørnsen Korollar~13.6.2. [Maybe reproduce the proof given LLN for completeness?]
\end{proof}
%
That is, given a sequence $(\rvar{X}_n)_{n\in\naturals}$, and one extra $\rvar{X}$, of i.i.d. random variables, the probability that $\rvar{X}$ lies in some Borel set $B$ can be thought of as the proportion of the $\rvar{X}_n$ that lie in $B$, as $n$ tends to infinity. In other words, probability is a measure of the \emph{frequency} with which an outcome of a random experiment obtains, if we repeat the experiment many times.

Whether or not this is the correct interpretation of probability as it occurs in the natural world we will not discuss here. Nonetheless the above result is an uncontroversial consequence of the theory, and it certainly aligns with our intuitive understanding of probability.

In this note we turn this result on its head and attempt to use it to motivate the formalisation of probability theory in terms of measure spaces. As we shall see, this is not entirely successful and will require some leaps that are not entirely justified by our conceptual grasp of probability.


\chapter{Preliminaries}


\section{Boolean algebras}

We begin by reviewing some of the purely algebraic properties of Boolean algebras.

\begin{definition}[Boolean algebras]
    \label{def:Boolean-algebra}
    A \emph{Boolean algebra} is a structure $\langle B; \join, \meet, ', 0, 1 \rangle$ such that
    %
    \begin{enumdef}
        \item $\langle B; \join, \meet \rangle$ is a distributive lattice,
        \item $0$ and $1$ are elements of $B$ such that $x \join 0 = x$ and $x \meet 1$ for all $x \in B$, and
        \item $'$ is a unary operation such that $x \join x' = 1$ and $x \meet x' = 0$ for all $x \in B$.
    \end{enumdef}
\end{definition}
%
The binary operations $\join$ and $\meet$ are called \emph{join} and \emph{meet}, respectively. For $x \in B$ the element $x'$ is called the \emph{complement} of $x$. In a general bounded lattice $L$, an element $y \in L$ such that $x \join y = 1$ and $x \meet y = 0$ is called a complement of $x \in L$. If $L$ is distributive, complements are unique. Recall also that the lattice structure on $B$ induces a partial order $\leq$ such that $x \leq y$ if and only if $x \join y = y$ for $x,y \in B$.

Let $B$ be a Boolean algebra. For $x,y \in B$ we define the \emph{symmetric difference} between $x$ and $y$ by
%
\begin{equation*}
    x \symdiff y
        = (x \meet y') \join (y \meet x').
\end{equation*}
%
If $x \symdiff y = 0$, then it is easy to show that $x = y$.

Before proceeding we note the following technical result that we shall need later:

\begin{lemma}
    \label{thm:refine_join}
    Let $\langle B; \join, \meet, ', 0, 1 \rangle$ be a Boolean algebra. Let $(x_i)_{i \in I}$ be a collection of elements in $B$ such that $x_i \meet x_j = 0$ when $i \neq j$. If $\bigjoin_{i \in I} x_i \in B$, then $\bigjoin_{i \in J} x_i \in B$ for any cofinite\footnotemark{} $J \subseteq I$.
\end{lemma}
\footnotetext{Recall that a subset $J$ of a set $I$ is called \emph{cofinite} if the complement $I \setminus J$ is finite.}

\begin{proof}
    Let $(x_i)_{i \in I}$ be such a collection of elements, and let $J \subseteq I$ be cofinite. It suffices to prove the lemma in the case $I \setminus J = \{i_0\}$, since the general case then follows by induction. We claim that
    %
    \begin{equation*}
        \bigjoin_{i \in J} x_i
            = x_{i_0}' \meet \bigjoin_{i \in I} x_i.
    \end{equation*}
    %
    Let $j \in J$ and notice that, since $x_{i_0} \meet x_j = 0$,
    %
    \begin{equation*}
        x_{i_0}' \meet x_j
            = (x_{i_0}' \meet x_j) \join (x_{i_0} \meet x_j)
            = (x_{i_0}' \join x_{i_0}) \meet x_j
            = 1 \meet x_j
            = x_j.
    \end{equation*}
    %
    Now because $x_j \leq \bigjoin_{i \in I} x_i$ we get
    %
    \begin{equation*}
        x_j
            = x_{i_0}' \meet x_j
            \leq x_{i_0}' \meet \bigjoin_{i \in I} x_i.
    \end{equation*}
    %
    Conversely, suppose that $x_j \leq s$ for all $j \in J$. Then $x_i \leq x_{i_0} \join s$ for all $i \in I$, so
    %
    \begin{equation*}
        \bigjoin_{i \in I} x_i
            \leq x_{i_0} \join s.
    \end{equation*}
    %
    It follows that
    %
    \begin{equation*}
        x_{i_0}' \meet \bigjoin_{i \in I} x_i
            \leq x_{i_0}' \meet (x_{i_0} \join s)
            = 0 \join (x_{i_0} \meet s)
            \leq s,
    \end{equation*}
    %
    as desired.
\end{proof}



\section{Abstract measure spaces}

\begin{definition}[Generalised abstract measure spaces]
    A \emph{measure} on a Boolean algebra $B$ is a map $\mu \colon B \to [0,\infty)$ such that $x \meet y = 0$ implies
    %
    \begin{equation}
        \label{eq:finite-additivity}
        \mu(x \join y)
            = \mu(x) + \mu(y)
    \end{equation}
    %
    for all $x,y \in B$. If $\mu$ is a measure on a Boolean algebra $B$, then we call the pair $(B,\mu)$ a \emph{generalised abstract measure space}. If $x \neq 0$ implies that $\mu(x) > 0$, then $\mu$ is called \emph{positive definite}. If $\mu(1) = 1$, then we call $\mu$ a \emph{probability measure}.
\end{definition}
%
It is clear that $\mu(\emptyset) = 0$ and that $\mu$ is increasing. It follows that $\mu(x) \leq \mu(1)$ for all $x \in B$. Notice that we require that $\mu$ is finite, but this is no restriction since we are ultimately interested in the case where $\mu$ is a probability measure.

The property \cref{eq:finite-additivity} is called \emph{(finite) additivity} of $\mu$, sine an easy induction argument extends it to all finite joins. We will later define more restrictive structures and measures upon them, hence the adjective \enquote{generalised}.

\begin{proposition}[Boole's inequality]
    Let $(B,\mu)$ be a generalised abstract measure space $B$. Then for any $x,y \in B$ we have
    %
    \begin{equation*}
        \mu(x \join y)
            \leq \mu(x) + \mu(y).
    \end{equation*}
\end{proposition}
%
Similar to \eqref{eq:finite-additivity}, Boole's inequality may be extended to all finite joins by induction.

\begin{proof}
    Notice that
    %
    \begin{equation*}
        (x \meet y') \meet y = x \meet (y' \meet y) = 0,
    \end{equation*}
    %
    and that
    %
    \begin{equation*}
        (x \meet y') \join y = (x \join y) \meet (y' \join y) = x \join y.
    \end{equation*}
    %
    It follows by additivity of $\mu$ that
    %
    \begin{equation*}
        \mu(x \join y)
            = \mu \bigl( (x \meet y') \join y \bigr)
            = \mu(x \meet y') + \mu(y)
            \leq \mu(x) + \mu(y),
    \end{equation*}
    %
    as desired.
\end{proof}


\begin{definition}[Metric Boolean algebras]
    A \emph{pseudometric Boolean algebra} is a tuple $(B,\rho)$, where $B$ is a Boolean algebra and $\rho$ is a pseudometric on $B$ such that the maps $x \mapsto x'$, $(x,y) \mapsto x \join y$, and $(x,y) \mapsto x \meet y$ are continuous.

    If $\rho$ is a metric, then $(B,\rho)$ is called a \emph{metric Boolean algebra}.
\end{definition}

Next we equip generalised abstract measure spaces with a canonical pseudometric. If $(B,\mu)$ is a generalised abstract measure space, define a map $\rho_\mu \colon B \times B \to [0,\infty)$ by
%
\begin{equation}
    \label{eq:Boolean-metric}
    \rho_\mu(x,y)
        = \mu(x \symdiff y)
\end{equation}
%
for $x,y \in B$. The next proposition shows that $\rho_\mu$ is in fact a pseudometric. We will always equip a generalised abstract measure space with this pseudometric.

\begin{proposition}
    Given a generalised abstract measure space $(B,\mu)$, the map $\rho_\mu$ defined in \eqref{eq:Boolean-metric} makes $(B,\rho_\mu)$ into a pseudometric Boolean algebra. Furthermore, $\rho_\mu$ is a metric if and only if $\mu$ is positive definite.
\end{proposition}

% Add to framedtheorems.sty
\newcommand{\mylistlabelfont}[1]{{\normalfont\color{linkcolor}\textit{#1}:}}
\newlist{proofsec}{description}{1}
\setlist[proofsec]{leftmargin=0pt, parsep=0pt, listparindent=\parindent, font=\mylistlabelfont}

\begin{proof}
\begin{proofsec}
    \item[$\rho_\mu$ is a pseudometric]
    We only need to prove the triangle inequality. To this end, let $x,y,z \in B$ and notice that
    %
    \begin{align*}
        x \meet z'
            &= (x \meet z) \meet (y' \join y) \\
            &= (x \meet z' \meet y') \join (x \meet z' \meet y) \\
            &\leq (x \meet y') \join (y \meet z').
    \end{align*}
    %
    Similarly we have $z \meet x' \leq (z \meet y') \join (y \meet x')$. It follows that
    %
    \begin{align*}
        x \symdiff z
            &= (x \meet z') \join (z \meet x') \\
            &\leq (x \meet y') \join (y \meet x') \join
                  (y \meet z') \join (z \meet y') \\
            &= (x \symdiff y) \join (y \symdiff z).
    \end{align*}
    %
    Now Boole's inequality implies that
    %
    \begin{equation*}
        \rho_\mu(x,z)
            = \mu(x \symdiff y)
            \leq \mu(x \symdiff y) + \mu(y \symdiff z)
            = \rho_\mu(x,y) + \rho_\mu(y,z),
    \end{equation*}
    %
    as desired.

    \item[Continuity of lattice operations]
    Let $x \in B$, and let $(x_n)_{n\in\naturals}$ be a sequence in $B$ that converges to $x$. Notice that $x_n' \symdiff x' = x_n \symdiff x$, so $\rho_\mu(x_n',x') = \rho_\mu(x_n,x)$. Hence the complementation map $x \mapsto x'$ is continuous.

    Let further $(y_n)_{n\in\naturals}$ be a sequence converging to a point $y \in B$. A short calculation shows that
    %
    \begin{align*}
        (x_n \join y_n) \symdiff (x \join y)
            &= (x_n \meet x' \meet y') \join
               (y_n \meet x' \meet y') \join
               (x \meet x_n' \meet y_n') \join
               (y \meet x_n' \meet y_n') \\
            &\leq (x_n \meet x') \join
            (x \meet x_n') \join
            (y_n \meet y') \join
            (y \meet y_n') \\
            &= (x_n \symdiff x) \join (y_n \symdiff y).
    \end{align*}
    %
    Thus Boole's inequality shows that
    %
    \begin{equation}
        \label{eq:join-continuous}
        \rho_\mu(x_n \join y_n, x \join y)
            \leq \rho_\mu(x_n,x) + \rho_\mu(y_n,y),
    \end{equation}
    %
    which implies continuity of the join map $(x,y) \mapsto x \join y$.

    Finally, continuity of the meet map $(x,y) \mapsto x \meet y$ follows since
    %
    \begin{equation*}
        x \meet y
            = \bigl( x' \join y' \bigr)',
    \end{equation*}
    %
    so it is a composition of continuous functions.

    \item[Positive definiteness]
    The last claim follows directly from the fact that $x \symdiff y = 0$ if and only if $x = y$ for all $x,y \in B$.
\end{proofsec}
\end{proof}

\begin{remark}
    \label{rem:convergence-of-measure}
    Notice that the measure $\mu$ can be written in terms of $\rho_\mu$, since $\mu(x) = \rho_\mu(x,0)$. Furthermore, since (pseudo)metrics are continuous, it follows that $\mu(x_n) \to \mu(x)$ whenever $x_n \to x$ in $B$.
\end{remark}


It is well-known that any (pseudo)metric space has a completion, i.e. can be isometrically embedded as a dense subset of a complete (pseudo)metric space. See for instance Corollary~24.5 in \textcite{willard}. A natural question is then: If $(B,\rho)$ is a (pseudo)metric Boolean algebra with metric completion $(\altoverline{B}, \altoverline{\rho})$, does $\altoverline{B}$ also carry the structure of a Boolean algebra?

This is indeed the case, and we sketch the construction: Let $x,y \in \altoverline{B}$, and let $(x_n)$ and $(y_n)$ be sequences in $B$ that converge to $x$ and $y$, respectively. Then these are Cauchy sequences in $B$, and the calculation leading to \eqref{eq:join-continuous} show that $(x_n \join y_n)$ is also a Cauchy sequence. Thus it converges to some element of $\altoverline{B}$. Denote it $x \join y$. We define $x'$ and $x \meet y$ similarly. It is easy to check that these operations satisfy the conditions in \cref{def:Boolean-algebra}. Furthermore, the completion $\altoverline{\rho}$ of the pseudometric $\rho$ makes $(\altoverline{B}, \altoverline{\rho})$ into a pseudometric Boolean algebra.

This takes care of the metric structure. The next proposition shows that we can also extend the measure on a generalised abstract measure space to its completion.

\begin{proposition}
    Let $(B,\mu)$ be a generalised abstract measure space, and let $(\altoverline{B}, \altoverline{\rho}_\mu)$ be the completion of $(B,\rho_\mu)$. Define a map $\altoverline{\mu} \colon \altoverline{B} \to [0,\infty)$ by
    %
    \begin{equation}
        \label{eq:mu-overline-def}
        \altoverline{\mu}(x) = \lim_{n\to\infty} \mu(x_n),
    \end{equation}
    %
    where $(x_n)_{n\in\naturals}$ is any sequence in $B$ that converges to $x$. Then $\altoverline{\mu}$ is a well-defined measure on $\altoverline{B}$. The generalised abstract measure space $(\altoverline{B}, \altoverline{\mu})$ is called the \emph{completion} of $(B,\mu)$.
\end{proposition}

\begin{proof}
    First notice that for any $x \in \altoverline{B}$ there does in fact exist a sequence in $B$ converging to $x$. If $(x_n)_{n\in\naturals}$ is such a sequence, it is a Cauchy sequence in $B$, and the reverse triangle inequality shows that $(\mu(x_n))$ is a Cauchy sequence in $\reals$, hence convergent. Thus the limit on the right-hand side of \eqref{eq:mu-overline-def} exists.
    
    Now let $(y_n)$ be another sequence in $B$ that approximates $x$. Another application of the reverse triangle inequality then shows that
    %
    \begin{equation*}
        \abs{ \mu(x_n) - \mu(y_n) }
            \leq \rho_\mu(x_n,y_n)
            \leq \rho_\mu(x_n,x) + \rho_\mu(y_n,x)
            \to 0.
    \end{equation*}
    %
    Hence $\mu(x_n)$ and $\mu(y_n)$ converge to the same value, and thus $\altoverline{\mu}$ is well-defined.

    Next we show that $\altoverline{\mu}$ is finitely additive. Let $x,y \in \altoverline{B}$ with $x \meet y = 0$ and choose approximating sequences $(x_n)$ and $(y_n)$ in $B$. Then
    %
    \begin{equation*}
        (x_n \join y_n) \meet (x_n \meet y_n)'
            = \bigl( x_n \meet (x_n \meet y_n)' \bigr)
              \join \bigl( y_n \meet (x_n \meet y_n)' \bigr)
    \end{equation*}
    %
    is the join of disjoint elements of $B$, so
    %
    \begin{equation*}
        \mu \bigl( (x_n \join y_n) \meet (x_n \meet y_n)' \bigr)
            = \mu \bigl( x_n \meet (x_n \meet y_n)' \bigr)
              + \mu \bigl( y_n \meet (x_n \meet y_n)' \bigr).
    \end{equation*}
    %
    By continuity of the lattice operations we have $(x_n \meet y_n)' \to 1$, so the three elements given as arguments to $\mu$ above are elements in approximating sequences for $x \join y$, $x$ and $y$ respectively. By definition of $\altoverline{\mu}$ it follows that
    %
    \begin{equation*}
        \altoverline{\mu}(x \join y)
            = \altoverline{\mu}(x) + \altoverline{\mu}(y)
    \end{equation*}
    %
    as desired.
\end{proof}


\begin{lemma}
    \label{thm:monotonic-sequence-Cauchy}
    Let $(B,\mu)$ be a generalised abstract measure space. Every monotonic sequence in $B$ is a Cauchy sequence.
\end{lemma}

\begin{proof}
    Let $(x_n)_{n\in\naturals}$ be an increasing sequence in $B$. Then since $x_n \leq 1$ we also have $\mu(x_n) \leq \mu(1) < \infty$ for all $n \in \naturals$. Thus the sequence $(\mu(x_n))$ is a bounded increasing sequence in $\setR$, hence it converges to some $\alpha \geq 0$. For $\epsilon > 0$ there is an $N \in \naturals$ such that $\mu(x_n) \in (\alpha - \epsilon, \alpha]$ for all $n \geq N$.
    
    If then $m,n \geq N$ with $m \leq n$, then $x_m \leq x_n$ and so
    %
    \begin{equation*}
        x_m \meet x_n'
            \leq x_n \meet x_n'
            = 0.
    \end{equation*}
    %
    Hence $x_m \symdiff x_n = x_n \meet x_m'$, so it follows that
    %
    \begin{equation*}
        \rho_\mu(x_m,x_n)
            = \mu(x_n \meet x_m')
            = \mu(x_n) - \mu(x_m)
            < \epsilon.
    \end{equation*}
    %
    Thus $(x_n)$ is indeed a Cauchy sequence. The case where $(x_n)$ is decreasing is similar.
\end{proof}


\begin{proposition}
    \label{thm:existence-of-joins}
    Let $(B,\rho)$ be a metric Boolean algebra with completion $(\altoverline{B}, \altoverline{\rho})$. Any sequence $(x_n)_{n\in\naturals}$ in $B$ has a join in $\altoverline{B}$, and
    %
    \begin{equation*}
        \bigjoin_{n\in\naturals} x_n
            = \lim_{n\to\infty} \bigjoin_{i \leq n} x_i.
    \end{equation*}
    %
    Similarly for meets.
\end{proposition}
%
As far as I know, it is not possible to generalise this result to the case where $\rho$ is only a pseudometric. [We also need the measure, don't we? For the lemma? Kolmogorov only looks at this when there is a positive definite measure.]

\begin{proof}
    The sequence $( \bigjoin_{i \leq n} x_i )_{n\in\naturals}$ is increasing, so by \cref{thm:monotonic-sequence-Cauchy} it has a limit $s \in \altoverline{B}$. For $k \in \naturals$ and $n \geq k$ we have $x_k \leq \bigjoin_{i \leq n} x_i$, i.e.
    %
    \begin{equation*}
        x_k \join \bigjoin_{i \leq n} x_i
            = \bigjoin_{i \leq n} x_i.
    \end{equation*}
    %
    Taking the limit as $n \to \infty$, continuity of (binary) joins implies that $x_k \join s = s$, or $x_k \leq s$. Thus $s$ is an upper bound of the sequence $(x_n)$.

    On the other hand, if $t \in \altoverline{B}$ is an upper bound of $(x_n)$, then $x_k \leq t$. We have just seen that taking limits preserves inequalities, so this implies that $s \leq t$ as desired.

    The corresponding result for meets follows similarly, or from the fact that complementation is continuous.
\end{proof}


\section[Abstract sigma-algebras and continuity][Abstract $\sigma$-algebras and continuity]{Abstract $\sigma$-algebras and continuity}

\begin{definition}[Axiom of continuity]
    A generalised abstract measure space $(B, \mu)$ is said to satisfy the \emph{axiom of continuity} if it has the following property: If $(x_n)_{n\in\naturals}$ is a decreasing sequence of elements in $B$ such that $\bigmeet_{n\in\naturals} x_n$ exists and equals $0$, then $\lim_{n\to\infty} \mu(x_n) = 0$.
\end{definition}
%
This is an analogue of the continuity (from above) of ordinary countably additive measures on concrete $\sigma$-algebras.


\begin{lemma}
    \label{thm:positive-definite-implies-continuous}
    Let $(B, \mu)$ be a generalised abstract measure space with $\mu$ positive definite. Then $(B,\mu)$ satisfies the axiom of continuity.
\end{lemma}

\begin{proof}
    Let $(x_n)_{n\in\naturals}$ be a decreasing sequence in $B$ such that $\bigmeet_{n\in\naturals} x_n = 0$. Since it is decreasing, $x_n = \bigmeet_{i \leq n} x_i$, so because $\mu$ is positive definite, \cref{thm:existence-of-joins} implies that the sequence converges to its meet, i.e. it converges to $0$. By \cref{rem:convergence-of-measure}, $\mu(x_n)$ converges to $\mu(0) = 0$, proving the claim.
\end{proof}


\begin{proposition}[The generalised addition theorem]
    \label{thm:generalised-addition}
    Let $(B, \mu)$ be a generalised abstract measure space satisfying the axiom of continuity. If $(x_n)_{n\in\naturals}$ is a sequence of pairwise disjoint elements in $B$ such that $x = \bigjoin_{n\in\naturals} x_n \in B$, then
    %
    \begin{equation*}
        \mu(x)
            = \sum_{n=1}^\infty \mu(x_n).
    \end{equation*}
\end{proposition}

\begin{proof}
    By \cref{thm:refine_join}, $r_n = \bigjoin_{i > n} x_i$ exists in $B$, and we claim that $\bigmeet_{n\in\naturals} r_n = 0$. Let $t \in \calF$ be a lower bound of $r_n$ for $n \in \naturals$. Then for $n \in \naturals$ we have
    %
    \begin{equation*}
        t \join \bigjoin_{i > n} x_i
            = \bigjoin_{i > n} x_i,
    \end{equation*}
    %
    and taking the meet of each side with $x_n'$ yields $t \meet x_n' = 0$. Hence $\bigjoin_{n\in\naturals} t \meet x_n = 0$. Now notice that $\bigmeet_{n\in\naturals} x_n = 0$, since $x_n \meet x_m = 0$ when $n \neq m$. It follows by taking complements that $\bigjoin_{n\in\naturals} x_n' = 1$, and so
    %
    \begin{equation*}
        t
            = t \meet \bigjoin_{n\in\naturals} x_n'
            = \bigjoin_{n\in\naturals} t \meet x_n'
            = 0.
    \end{equation*}
    %
    Thus $\bigmeet_{n\in\naturals} r_n = 0$ as claimed. It now follows from finite additivity of $\mu$ and the axiom of continuity that
    %
    \begin{equation*}
        \mu(x)
            = \sum_{i=1}^n \mu(x_i) + \mu(r_n)
            \to \sum_{i=1}^\infty \mu(x_i)
    \end{equation*}
    %
    as $n \to \infty$ as desired.
\end{proof}


\begin{proposition}[Boole's inequality]
    Let $(B,\mu)$ be a generalised abstract measure space, and let $(x_n)_{n\in\naturals}$ be a sequence in $B$. If $x = \bigjoin_{n\in\naturals} x_n \in B$, then
    %
    \begin{equation*}
        \mu(x)
            \leq \sum_{n=1}^\infty \mu(x_n).
    \end{equation*}
\end{proposition}

\begin{proof}
    First notice that, by the finite Boole equality,
    %
    \begin{equation*}
        \mu \Bigl( \bigjoin_{i \leq n} x_i \Bigr)
            \leq \sum_{i=1}^n \mu(x_i)
            \leq \sum_{i=1}^\infty \mu(x_i)
    \end{equation*}
    %
    for all $n \in \naturals$. By \cref{thm:existence-of-joins}, $\bigjoin_{i \leq n} x_i \to x$ as $n \to \infty$, so $\mu( \bigjoin_{i \leq n} x_i ) \to \mu(x)$. The claim follows
\end{proof}


Of course, the join of a sequence of elements may not exist. In the case where we are ensured the existence of countable joins we use the following terminology:

\begin{definition}[Abstract $\sigma$-algebra]
    \label{def:abstract-sigma-algebra}
    An \emph{abstract $\sigma$-algebra} is a Boolean algebra $B$ with countable joins. That is, if $(x_n)_{n\in\naturals}$ is a sequence of elements in $B$, then their join $\bigjoin_{n\in\naturals} x_n$ exists.
\end{definition}
%
If the join $\bigjoin_{n\in\naturals} x_n$ exists, then it follows by taking complements that the meet $\bigmeet_{n\in\naturals} x_n'$ also exists. Hence an abstract $\sigma$-algebra also has countable meets. In the context of abstract measure spaces we obtain the following:

\begin{definition}[Abstract measure spaces]
    \label{def:abstract-measure-space}
    An \emph{abstract measure space} is a generalised abstract measure space $(B,\mu)$ that satisfies the axiom of continuity, and where $B$ is an abstract $\sigma$-algebra.
\end{definition}

\begin{lemma}
    \label{thm:completion-gives-countable-additivity}
    If $(B,\mu)$ is a generalised abstract measure space with $\mu$ positive definite, then the completion $(\altoverline{B},\altoverline{\mu})$ is an abstract measure space.
\end{lemma}

\begin{proof}
    Since the completion of a metric space is a metric space, $\altoverline{\mu}$ is positive definite, so \cref{thm:positive-definite-implies-continuous} implies that $(\altoverline{B},\altoverline{\mu})$ satisfies the axiom of continuity.

    On the other hand, \cref{thm:existence-of-joins} implies that every sequence in $\altoverline{B}$ has a join, so $\altoverline{B}$ is an abstract $\sigma$-algebra.
\end{proof}



\chapter{The algebra of probability spaces}

\section{Motivation}

\newcommand{\compl}[1]{\altoverline{#1}}

If the probability of an event is supposed to be a measure of how often this event occurs on repetitions of the random experiment in question, then it seems reasonable to assume that we are, at least in principle, able to distinguish when the event does and does not obtain. For example, after rolling a six-sided die the state of affairs \textquote{the result of the die roll is three} is an event, since we can determine the outcome of the roll just by looking at the die. To take another example, after throwing a ball the state of affairs \textquote{the ball was thrown more than 50 metres} is also an event: That is, we can determine whether or not the length of the throw was strictly greater than 50 metres.

One might take a different view: Say that one grants that it is possible to \emph{affirm} that the length of the throw, measured in metres, lies in the interval $(50,\infty)$. If the length $L$ in metres does in fact lie in the above interval, we can simply use a ruler whose subdivisions are smaller than $L - 50$ in metres. Still one might disagree that it is possible to \emph{refute} that $L \in (50, \infty)$. For if $L$ is exactly $50$ metres, then since any measurement of $L$ carries some error, it is in practice impossible to determine whether $L$ is $50$ (or slightly smaller), or whether it is slightly larger than $50$. We will not pursue this line further but refer the reader to \textcite{vickers1989} for more on this \emph{logic of affirmative assertions}.

To be precise, after performing the relevant random experiment, we will assume that we are always able to decide whether or not the event has occurred or not. In particular, if $E$ is an event, then the state of affairs \textquote{$E$ does not obtain} is also an event, denoted $\compl{E}$: If $E$ obtains, then $\compl{E}$ does not. And conversely, if $E$ does not obtain, then $\compl{E}$ does obtain. We call $\compl{E}$ the \emph{complement of} or the \emph{complementary event to $E$}.\footnote{In contrast, in the logic of affirmative assertions we do not allow complementation (i.e. negation). Hence it may not be surprising that this logic ends up being closely tied to topology, the relevant analogy being that the complement of an open set need not be open.} Evidently, the complement of a complement is just the event we started with.

Next consider two events $E_1$ and $E_2$. Since we are able to decide whether each of them have obtained, the same is true for the event \textquote{both $E_1$ and $E_2$ have obtained} and the event \textquote{at least one of $E_1$ and $E_2$ has obtained}. The first is called the \emph{conjunction} of $E_1$ and $E_2$ and is denoted $E_1 \meet E_2$, and the second is the \emph{disjunction} $E_1 \join E_2$ of $E_1$ and $E_2$.

Finally it seems natural to allow an \textquote{impossible event} $0$ which never occurs, as well as a \textquote{sure event} $1$ that always occurs. Clearly $0$ and $1$ are each other's complements. If $E_1$ and $E_2$ are events with $E_1 \meet E_2 = 0$, then this manifestly means that $E_1$ and $E_2$ cannot obtain simultaneously: The two events are \emph{incompatible}.

We collect all the relevant events in a set $\calF$ and postulate that the structure $\langle \calF; \join, \meet, \compl{\,\cdot\,\vphantom{e}}, 0, 1 \rangle$ is a Boolean algebra. We leave it to the reader to reflect on the reasonability of this assumption. This leads naturally to the following definition:

\begin{definition}[Generalised abstract probability spaces]
    A \emph{generalised abstract probability space} is a generalised abstract measure space $(\calF, \P)$, where $\P$ is a positive definite probability measure.
\end{definition}
%
The partial order $\leq$ induced by the lattice structure then has the interpretation that if $E \leq F$, then $E$ implies $F$. For recall that this means that $E \join F = F$, i.e. if $F$ has already occurred then no information is gained by observing that $E$ has also occurred. Conversely, if $F$ has \emph{not} occurred, then $E$ is impossible.

We have justified every part of this definition except for the positive definiteness of $\P$. In the usual measure theoretical formulation of probability theory, there may (and often do) exist events that are not empty but still have probability zero. Kolmogorov had the following to say in critique of this approach:
%
\blockquote[\cite{kolmogorov1995}]{%
    \textins*{W}e are forced to give up the principle, formulated in numerous classical works in probability theory, according to which an event of probability zero is absolutely impossible. More precisely, one must allow that an event of positive probability can be decomposed into a (possible continuous) infinity of variants of which each has probability
    zero.%
}
%
Furthermore, this also has the technical benefit that the pseudometric $\rho_\P$ induced by $\P$ is in fact a metric. We will return to the consequences of this choice in the next section.


\section{Continuity of probability measures}

Of course, the map $\P$ above is supposed to be analogous to a probability measure on a (concrete) $\sigma$-algebra. But ordinary measures are \emph{countably} additive, not just finitely so. It is however difficult to justify extending the finite additivity to sequences of disjoint events purely on conceptual or operational groups. In fact, according to Kolmogorov himself:
%
\blockquote[\cite{kolmogorov1956}]{%
    Since the new axiom \textins{countable additivity} is essential for infinite fields of probability only, it is almost impossible to elucidate its empirical meaning. \textelp{} For, in describing any observable random process we can obtain only finite fields of probability. Infinite fields of probability occur only as idealized models of real random processes. \emph{We limit ourselves, arbitrarily, to only those models which satisfy Axiom VI} \textins{countable additivity}. This limitation has been found expedient in researches of the most diverse sort.%
}
%
And furthermore:
%
\blockquote[\cite{kolmogorov1995}]{%
    \textins*{S}omewhat more complicated problems require, if the theory is to be simple and tractable, that probability be subject to the \emph{axiom of denumerable additivity}. However, the justification of that axiom remains purely empirical, in that we have not yet encountered any interesting problem for which we have not been able to construct a probability field conforming to the axiom in question.%
}
%
The axiom of continuity is, as we saw in \cref{thm:generalised-addition}, equivalent to countable additivity in our formalism. While countable additivity is usually preferred in the definition of (probability) measures today, \textcite{kolmogorov1956} instead assumed the axiom of continuity and then proved the generalised addition theorem, as we have done above.

Kolmogorov could apparently only find justification for assuming the axiom in the success of the theory, and not in its conceptual underpinnings. Luckily, in the present context we can avoid taking a stance: We have already heard Kolmogorov argue that the absense of non-empty events of probability zero is conceptually problematic; hence we have disallowed them by assuming our probability measures to be positive definite. And according to \cref{thm:positive-definite-implies-continuous}, in this setting we get the axiom of continuity for free.

We are still not assured that our probability spaces are closed under countable joins. However, \cref{thm:completion-gives-countable-additivity} tells us that taking the completion of a generalised abstract probability space solves the problem. And according to Kolmogorov, this can be done without issue:
%
\newlength{\oldparindent}
\setlength{\oldparindent}{\parindent}
\blockquote[\cite{kolmogorov1995}]{\setlength{\parskip}{0pt}\setlength{\parindent}{\oldparindent}
    The use of denumerable additivity, with its great concomitant freedom, is not generally possible except in \emph{complete} metric Boolean algebras. It is therefore natural, in probability theory, always to assume that the algebra of events is \emph{complete} by adjunction of ideal elements. As has been pointed out, this situation is always realizable.

    So, the use of denumerably additive probabilities is seen to be legitimate and to impose no additional restrictions on the nature of the problems which fall under the scope of the general theory.
}
%
Notice that he calls the adjoined elements \enquote{ideal}. Thus he seems to think of these new events as not (necessarily) corresponding to a real, observable event as described above. Instead, he seems only to be concerned with how their adjunction can improve the flexibility of the theory without restricting which classes of problems the theory can address. And indeed, upon taking the completion of a probability space we always have an isometric copy of the original space inside the new one.

We summarise the result of the above discussion in the following definition, which is the one we will be concerned with going forward:

\begin{definition}[Abstract probability spaces]
    \label{def:abstract-probability-space}
    An \emph{abstract probability space} is a generalised abstract probability space $(\calF, \P)$, where $\calF$ is an abstract $\sigma$-algebra.
\end{definition}


\chapter{Set-theoretical probability theory}

\section{Basic definitions and properties}

We begin by recalling the standard definitions in order to compare them to the abstract versions we have considered above:

\begin{definition}[Measurable spaces]
    A \emph{(concrete) $\sigma$-algebra} in a set $\Omega$ is a collection $\calF$ of subsets of $\Omega$ such that
    %
    \begin{enumdef}
        \item $\Omega \in \calF$,
        \item $A \in \calF$ implies $A^c \in \calF$, and
        \item if $(A_n)_{n\in\naturals}$ is a sequence in $\calF$, then $\bigunion_{n\in\naturals} \in \calF$.
    \end{enumdef}
    %
    The sets in $\calF$ are called \emph{$\calF$-measurable}, and the pair $(\Omega, \calF)$ is called a \emph{measurable space}.
\end{definition}
%
Note that a $\sigma$-algebra by this definition is indeed an \emph{abstract} $\sigma$-algebra as defined in \cref{def:abstract-sigma-algebra}. Also contrast the definition of a $\sigma$-algebra with that of a \emph{set algebra}: in the latter case also require it to contain $\Omega$ and be closed under complementation, but we only require it to be closed under \emph{finite} intersections. In particular a set algebra is a Boolean algebra.

\begin{enumdef}
    \item \verb|\itemsep:| \the\itemsep
    \item \verb|\labelsep:| \the\labelsep
    \item \verb|\labelwidth:| \the\labelwidth
    \item \verb|\parsep:| \the\parsep
    \item \verb|\topsep:| \the\topsep
    \item \verb|\partopsep:| \the\partopsep
    \item \verb|\leftmargin:| \the\leftmargin
\end{enumdef}


\begin{definition}[Measure spaces and probability spaces]
    Let $(\Omega, \calF)$ be a measurable space. A \emph{measure} on $(\Omega, \calF)$ is a map $\mu \colon \calF \to [0,\infty]$ such that
    %
    \begin{enumdef}
        \item $\mu(\emptyset) = 0$, and
        \item $\mu$ is countably additive, i.e. for every sequence $(A_n)_{n\in\naturals}$ of pairwise disjoint sets in $\calF$ we have
        %
        \begin{equation*}
            \mu \Bigl( \bigunion_{n\in\naturals} A_n \Bigr)
                = \sum_{n=1}^\infty \mu(A_n).
        \end{equation*}
    \end{enumdef}
    %
    The triple $(\Omega, \calF, \mu)$ is called a \emph{(concrete) measure space}. If $\mu(\Omega) = 1$ then $\mu$ is called a \emph{probability measure} and the triple $(\Omega, \calF, \mu)$ a \emph{(concrete) probability space}.
\end{definition}
%
Again, if $(\Omega, \calF, \mu)$ is a measure space thus defined, then $(\calF, \mu)$ is also an \emph{abstract} measure space of the type considered in \cref{def:abstract-measure-space}. However, even if $\mu$ is a probability measure, $(\calF, \mu)$ is not necessarily an \emph{abstract} probability space as defined in \cref{def:abstract-probability-space}.

The trouble is that $\mu(A) = 0$ does not necessarily imply that $A = \emptyset$, i.e. $\mu$ need not be positive definite. In an abstract measure space (perhaps of the generalised kind) $(\calF,\mu)$ the only information we have about the relationship between two elements $x,y \in \calF$ is whether they are comparable and, if so, which one is greater.\footnote{Recall that the ordering on a lattice completely determines the lattice structure.} Or in other terms for events $E$ and $F$, whether one of them implies the other or not.

By contrast, elements of a concrete $\sigma$-algebra are subsets of an underlying set. This has several important implications: First of all, this vastly increases the number of objects we have to contend with, namely \emph{every} subset of $\Omega$, not just those lying in $\calF$. This may have benefits of the technical nature; certainly it would have simplified many of the arguments in the previous section on Boolean algebras.

Secondly, it forces us to take the very elements of the \emph{sample space} $\Omega$ seriously. Usually these are referred to as \emph{outcomes} or, e.g. by Kolmogorov, \emph{elementary events}. But this may be conceptually problematic:
%
\blockquote[\cite{kolmogorov1995}]{
    \textins*{T}he notion of an elementary event is an artificial superstructure imposed on the concrete notion of an event. In reality, events are not composed of elementary events, but elementary events originate in the dismemberment of composite events.
}
%
In other words, arriving at the idea of an elementary event requires \emph{analysis} of the collection of events that is given in some random experiment.

A simple example may help elucidate this point: Consider the random experiment consisting of rolling a six-sided die. Say this die is loaded in such a way that it is impossible to roll a $6$, and that it is very likely to roll a $1$ or a $2$. We construct an abstract probability space ($\calF, \P)$ to model the outcome of the die roll. The event space might be
%
\begin{equation*}
    \calF
        = \{ 0, A_1, \ldots, A_5, E, F, 1 \},
\end{equation*}
%
where we interpret the event $A_i$ as \enquote{the result was $i$} for $i = 1, \ldots, 5$, $E$ as \enquote{the result was even} and $F$ as \enquote{the result was odd}. There is no event $A_6$ since it is impossible to roll a $6$. Certainly we would require that $A_1, A_3, A_5 \leq F$, and that $A_2, A_4 \leq E$. Assigning probabilities to each event we might find that $\P(A_1) = \P(A_2) = 0.35$, and that $\P(A_3) = \P(A_4) = \P(A_5) = 0.1$. Additivity of $\P$ would then imply that $\P(E) = 0.45$ and $\P(F) = 0.55$.

It is tempting to \emph{identify} each of $E$ and $F$ with the (possible) events that imply them, i.e. identify $E$ with the set $\{A_2, A_4\}$ and $F$ with the set $\{A_1, A_3, A_4\}$ of events. Maybe we are even tempted to include a hypothetical event $A_6$ in the former set. But notice that this is an \emph{analysis} of the events $E$ and $F$. It would be quite possible to grasp the meaning of the event $E$ without immediately enumerating each of the \enquote{elementary} events it consists of. Indeed events in even set-theoretic probability theory (and objects in mathematics as a whole, for that matter) are often defined in terms of their properties, not of their constituents. Or to put it in other terms: by their \emph{intension} and not their \emph{extension}.

On the other hand, the set-theoretic approach and the possibility of non-empty null sets offer certain conceptual advantages as well. Perhaps we would like there to be an event $A_6$ even if it is impossible. Certainly our model of the die seems incomplete without it; the die is, after all, six-sided! Admittedly this picture seems rather artificial since the probability of rolling a $6$ using a physical die is surely positive, if very small. So imagine that this die appears in a video game or in a fantasy novel where this might be a more easily digestible proposition.

Furthermore, if we actually do want to use our theory of probability to \emph{model} physical phenomena, then it might be perfectly reasonable to include some event in the model on conceptual grounds, even if this event happens to be assigned probability zero in the model. Perhaps this fact is due to numerical approximation, missing information or a state of affairs that comes about after each event under consideration has been identified.


\section[Extending premeasures to sigma-algebras][Extending premeasures to $\sigma$-algebras]{Extending premeasures to $\sigma$-algebras}

Recall that we in \cref{thm:completion-gives-countable-additivity} proved that the completion of a generalised abstract measure space $(B,\mu)$ is an abstract measure space under the assumption that $\mu$ is positive definite. Since this is no longer the case we need another way of extending a finitely additive probability measure on a set algebra to a $\sigma$-algebra. Furthermore, in \cref{thm:positive-definite-implies-continuous} we showed that $(B,\mu)$ automatically satisfies the axiom of continuity if $\mu$ is positive definite. We will thus also have to overcome this obstacle.

% Kolmogorov, now introduces axiom of continuity. Premeasures and Caratheodory, no issue if we assume the axiom! Conceptual reasons for assuming this?
% Rename section, perhaps something about continuity.

We begin with the latter: Let $\calA$ be an algebra on a set $\Omega$, and let $\mu_0 \colon \calA \to [0,\infty]$ be a \emph{premeasure} on $\calA$. Recall that this means that $\mu_0(\emptyset) = 0$, and that
%
\begin{equation*}
    \mu_0 \bigl( \bigunion_{n\in\naturals} A_n \bigr)
        = \sum_{n=1}^\infty \mu_0(A_n)
\end{equation*}
%
for any sequence $(A_n)_{n\in\naturals}$ of pairwise disjoint sets in $\calA$ such that $\bigunion_{n\in\naturals} A_n \in \calA$. Notice that if $\mu_0$ is finite, this says exactly that $(\calA,\mu_0)$ is a generalised abstract measure space. But if $\mu_0$ is finite, then countable additivity of the above sort follows from \cref{thm:generalised-addition} if only $(\calA,\mu_0)$ satisfies the axiom of continuity.

We will attempt to motivate the axiom of continuity for a generalised abstract probability space $(\calF, \P)$. In this setting the axiom says that, given a decreasing sequence of events $(E_n)_{n\in\naturals}$ in $\calF$ such that $\bigmeet_{n\in\naturals} E_n = 0$, we have $\lim_{n\to\infty} \P(E_n) = 0$.

If $(E_n)$ is eventually constant, $E_n$ it must equal $0$ for large enough $n$, in which case the axiom is obvious. If instead all $E_n$ are possible, the assumption that $\lim_{n\to\infty} \P(E_n) = 0$ says that it is still impossible for all $E_n$ to obtain. Since the sequence is decreasing, this must mean that the $E_n$ become increasingly less likely to occur, and the probability that $E_n$ occurs must get vanishingly small as $n$ tends to infinity. In other words, $\P(E_n)$ must approach zero in the limit $n \to 0$.

Of course this is not a proof of the axiom of continuity, but it illustrates that it a quite natural assumption. Thus we will henceforth assume that $\mu_0$ is indeed a premeasure on a set algebra $\calA$.

Next, recall that if $\calJ$ is a collection of subsets of $\Omega$ containing $\emptyset$ and $\mu \colon \calJ \to [0,\infty]$ satisfies $\mu(\emptyset) = 0$, then
%
\begin{equation}
    \label{eq:outer-measure}
    \mu^*(A)
        = \inf \set[\Big]{
            \sum_{n=1}^\infty \mu(B_n)
        }{
            (B_n)_{n \in \naturals} \subseteq \calJ
            \text{ and }
            A \subseteq \bigunion_{n \in \naturals} B_n
        }
\end{equation}
%
defines an outer measure on $\Omega$. We denote the $\sigma$-algebra of $\mu^*$-measurable sets by $\calM(\mu^*)$. In the case $\calJ = \calA$ and $\mu = \mu_0$, the following theorem guarantees that we may extend $\mu_0$ to an almost unique countably additive measure: (Folland 1.14)

\begin{theorem}
    Let $\mu_0$ be a premeasure on an algebra $\calA$ in a set $\Omega$, and let $\mu^*$ be given by \cref{eq:outer-measure} with $\mu = \mu_0$ and $\calJ = \calA$. Then $\calA \subseteq \calM(\mu^*)$, and $\mu^*|_\calA = \mu_0$.

    In particular, $\mu^*$ restricts to a measure $\mu$ on $\calF = \sigma(\calA)$ whose restriction to $\calA$ is $\mu_0$. If $\nu$ is another measure on $\calF$ that extends $\mu_0$, then $\nu(A) \leq \mu(A)$ for all $A \in \calF$, with equality when $\mu(A) < \infty$. If $\mu_0$ is $\sigma$-finite, then $\mu = \nu$.
\end{theorem}

\begin{proof}
    Folland Theorem~1.14.
\end{proof}
%
Thus in the case where $\mu_0$ is a probability premeasure, there exists a \emph{unique} extension to a probability measure $\P$ on the $\sigma$-algebra $\calF$ generated by $\calA$.

To sum up: Given a finitely additive probability measure $\mu_0$ on a set algebra $\calA$ in a set $\Omega$, the pair $(\calA,\mu_0)$ is a generalised abstract measure space. However, since $\mu_0$ is not necessarily positive definite, it does not immediately extend to a measure on an abstract $\sigma$-algebra, the issue being that $(\calA,\mu_0)$ does not satisfy the axiom of continuity. Thus we must impose this axiom, and we have argued that this is reasonable. Under this assumption $\mu_0$ becomes a premeasure and thus extends uniquely to a measure $\P$ on $\calF = \sigma(\calA)$.

Thus we arrive at the usual conception of a probability space as a measure space $(\Omega,\calF,\P)$ with $\P$ a countably additive probability measure. However, we still don't really understand how to think about $\sigma$-algebras. Can we interpret $\sigma$-algebras in some way, or are they merely a convenience that doesn't mean anything in practice?

% Use DeclarePairedDelimiter to get delimiter sizes
\newcommand{\condP}[2]{\P(#1 \mid #2)}

\section{Conditional probability and independence}

Let $(\Omega,\calF,\P)$ be a probability space. Given events $A,B \in \calF$ with $\P(B) > 0$, the \emph{conditional probability} of $A$ given $B$ is defined as
%
\begin{equation*}
    \condP{A}{B}
        = \frac{ \P(A \intersect B) }{ \P(B) }.
\end{equation*}
%
Notice that the map $A \mapsto \condP{A}{B}$ is a probability measure on $B$. We interpret it as the probability that $A$ obtains given the knowledge that $B$ has already occurred. In the case that knowing that $B$ has occurred yields no knowledge about the likelihood of $A$, we would expect that $\condP{A}{B} = \P(A)$, or equivalently
%
\begin{equation*}
    \P(A \intersect B) = \P(A) \P(B).
\end{equation*}
%
That is, $A$ and $B$ are \emph{independent}:

\begin{definition}[Independence I]
    \label{def:independence-1}
    Let $I$ be a non-empty index set. A family $(A_i)_{i \in I}$ of events from $\calF$ is called \emph{independent} relative to $\P$ if
    %
    \begin{equation*}
        \P \Bigl( \bigintersect_{\nu = 1}^n A_{i_\nu} \Bigr)
            = \bigprod_{\nu = 1}^n \P(A_{i_\nu}),
    \end{equation*}
    %
    for any finite subset $\{ i_1, \ldots, i_n \}$ of distinct elements of $I$.
\end{definition}
%
The intuition being that the knowledge that some $A_i$ has obtained does not affect our knowledge of the other $A_i$. We may generalise this definition in the following way:

\begin{definition}[Independence II]
    Let $(\calC_i)_{i \in I}$ be a family of sets $\calC_i \subseteq \calF$ of events. This family is called \emph{independent} if
    %
    \begin{equation*}
        \P \Bigl( \bigintersect_{\nu = 1}^n A_{i_\nu} \Bigr)
            = \bigprod_{\nu = 1}^n \P(A_{i_\nu}),
    \end{equation*}
    %
    for any choice of events $A_{i_\nu} \in \calC_{i_\nu}$ and any finite subset $\{ i_1, \ldots, i_n \}$ of distinct elements of $I$.
\end{definition}

\begin{remark}
    \label{rem:finite-subfamilies-independent}
    Finite subfamilies independent is sufficient.
\end{remark}
%
Notice that this definition reduces to the first one if all $\calC_i$ are singletons. But notice what this definition says: Choosing a single event $A_i$ from each $\calC_i$, these $A_i$ are independent. The definition does not imply any relationship between the events in each $\calC_i$. An example, adapted from Bauer prob Example~6.1, will illustrate this point:

\newcommand{\powerset}[1]{2^{#1}}

\begin{example}
    \label{ex:die-throw-1}
    Consider two throws of a fair die. The sample space is then $\Omega = \{1, \ldots, 6\}^2$, and we consider the probability space $(\Omega, \powerset{\Omega}, \P)$, where $\P$ assigns equal probability $1/6$ to each elementary event $\{(i,j)\}$. Let $A_1$ and $A_2$ be the events that on the first throw an even or odd number showed up, respectively, and let $A_3$ be the event that the sum of the two throws is odd. Now collect the three events in $\calC_1 = \{A_1, A_2\}$ and $\calC_2 = \{A_3\}$. Then $\calC_1$ and $\calC_2$ are independent even through $A_1$ and $A_2$ are not.
\end{example}
%
The question then becomes, why are we interested in this generalisation of independence? Independence of two collections $\calC_1$ and $\calC_2$ is supposed to mean that the information contained in each is in some way independent of the information contained in the other. By \enquote{information} we mean something like: the ability to better predict the outcome of a random experiment, or in other words improved knowledge of the probabilities of events.

Given that $\calC_1$ and $\calC_2$ are independent, is there any more we can say? Is it the case, for instance, that each event $\calC_1$ is independent of the (finite or countable) intersection of events in $\calC_2$? What about unions or complements? In the case of intersections the answer is negative:

\begin{example}
    \label{ex:die-throw-2}
    Let $(\Omega,\powerset{\Omega},\P)$ be the probability space defined in \cref{ex:die-throw-1}, and let $A_1$ and $A_3$ be the same events as before. But now let $A_2'$ be the event that the \emph{second} throw yielded an odd number, and put $\calC_1' = \{A_1, A_2'\}$. Again $\calC_1'$ and $\calC_2$ are easily seen to be independent, and indeed so are $A_1$ and $A_2'$. However, $A_1 \intersect A_2'$ and $A_3$ are manifestly not independent: For while $\P(A_1 \intersect A_2') = 1/4$ by independence, we have
    %
    \begin{equation*}
        \P \bigl( (A_1 \intersect A_2') \intersect A_3 \bigr)
            = 0,
    \end{equation*}
    %
    since if each throw turns up odd, the sum must be even.

    Incidentally, this also shows that the requirement in \cref{def:independence-1} that $\P$ be multiplicative for \emph{every} finite subset of $I$ is necessary; it is not enough that the events be \emph{pairwise} independent.
\end{example}

As for unions and complementation, we have slightly more success. To do this discussion justice we introduce another piece of terminology:

\begin{definition}[Dynkin systems]
    A collection $\calD$ of subsets of a set $X$ is called a \emph{Dynkin system} in $X$ if
    %
    \begin{enumdef}
        \item $X \in \calD$,
        \item $B \setminus A \in \calD$ for $A,B \in \calD$ with $A \subseteq B$, and
        \item $\bigunion_{n\in\naturals} A_n \in \calD$ for every increasing sequence $(A_n)_{n\in\naturals}$ of sets in $\calD$.
    \end{enumdef}
\end{definition}
%
A Dynkin system is also variously called a Dynkin class, $\delta$-system, $d$-system, or $\lambda$-system. Clearly every $\sigma$-algebra is a Dynkin system. If $\calS$ is a collection of subsets of $X$, then there is a smallest Dynkin system in $X$ that contains $\calS$, namely the intersection of all such Dynkin systems. We denote this by $\delta(\calS)$ and say that it is \emph{generated} by $\calS$.

The motivation for considering Dynkin systems is twofold. First of all they are significantly simpler than $\sigma$-algebras, and working with Dynkin systems instead of $\sigma$-algebras can often we done with no loss of generality, as the following fundamental result shows:

\begin{theorem}[Dynkin's Lemma]
    Let $\calS$ be a collection of subsets of a set $X$ that is closed under finite intersections. Then
    %
    \begin{equation*}
        \delta(\calS) = \sigma(\calS).
    \end{equation*}
\end{theorem}
%
Also known as \emph{Dynkin's $\pi$-$\lambda$ theorem} since a non-empty collection of sets that is closed under finite intersections is also called a $\pi$-system.

\begin{proof}
    Bauer Theorem~2.3 (different definition of Dynkin systems), Cohn Theorem~1.6.2.
\end{proof}

Another source of motivation comes from the following result about finite measures which is important when proving uniqueness of properties of finite or $\sigma$-finite measures, but will also be of interest to us below:

\begin{lemma}
    \label{thm:finite-measures-agree-on-Dynkin}
    Let $\mu$ and $\nu$ be finite measures on a measurable space $(X,\calE)$ such that $\mu(X) = \nu(X)$. The family $\calD \subseteq \calE$ of sets on which $\mu$ and $\nu$ agree is a Dynkin system.
\end{lemma}

\begin{proof}
    By assumption $X \in \calD$. Let $A_1, A_2 \in \calD$ with $A_1 \subseteq A_2$. Then
    %
    \begin{equation*}
        \mu(A_2 \setminus A_1)
            = \mu(A_2) - \mu(A_1)
            = \nu(A_2) - \nu(A_1)
            = \nu(A_2 \setminus A_1),
    \end{equation*}
    %
    since $\mu$ and $\nu$ are finite. Finally assume that $(A_n)_{n\in\naturals}$ is an increasing sequence of elements in $\calD$. Then by continuity we have
    %
    \begin{equation*}
        \mu \Bigl( \bigunion_{n\in\naturals} A_n \Bigr)
            = \lim_{n\to\infty} \mu(A_n)
            = \lim_{n\to\infty} \nu(A_n)
            = \nu \Bigl( \bigunion_{n\in\naturals} A_n \Bigr).
    \end{equation*}
    %
    Thus $\calD$ is a Dynkin system as claimed.
\end{proof}

Our motivation for considering Dynkin systems, however, is the following result:

\begin{proposition}
    \label{thm:Dynkin-independence}
    Let $(\calC_i)_{i \in I}$ be an independent family of sets of events from $\calF$. Then the family $(\delta(\calC_i))_{i \in I}$ is also independent. In particular, if the $\calC_i$ are closed under intersection, the family $(\sigma(\calC_i))_{i \in I}$ is independent.
\end{proposition}

\begin{proof}
    By \cref{rem:finite-subfamilies-independent} we may assume that $I$ is finite.

    Fix an index $i_0 \in I$, and choose sets $A_{i_\nu} \in \calC_{i_\nu}$ for distinct indices $i_1, \ldots, i_n \in I \setminus \{i_0\}$. Then define measures $\P_1$ and $\P_2$ on $\calF$ by
    %
    \begin{equation*}
        \P_1(A)
            = \P \Bigl( A \intersect \bigintersect_{\nu = 1}^n A_{i_\nu} \Bigr)
        \quad \text{and} \quad
        \P_2(A)
            = \P(A) \bigprod_{\nu = 1}^n \P(A_{i_\nu}),
    \end{equation*}
    %
    for $A \in \calF$. Notice that $\P_1$ and $\P_2$ agree on $\calC_{i_0}$ by independence, so since $\P_1(\Omega) = \P_2(\Omega)$, \cref{thm:finite-measures-agree-on-Dynkin} implies that they also agree on $\delta(\calC_{i_0})$. It follows that
    %
    \begin{equation*}
        \P \Bigl( A \intersect \bigintersect_{\nu = 1}^n A_{i_\nu} \Bigr)
            = \P(A) \bigprod_{\nu = 1}^n \P(A_{i_\nu})
    \end{equation*}
    %
    for all choices of sets $A_{i_\nu} \in \calC_{i_\nu}$. But this precisely expresses the independence of the family $(\calC_i)_{i \in I}$ with $\calC_{i_0}$ replaced by $\delta(\calC_{i_0})$.

    Performing a finite number of such replacements, once for each index in $I$, proves the first claim. The second claim follows by Dynkin's lemma.
\end{proof}


\section[Sigma-algebras and information][$\sigma$-algebras and information]{$\sigma$-algebras and information}

We interpret \cref{thm:Dynkin-independence} as follows: Say that we are interested in an event $A \in \calC_1$. The independence of $\calC_1$ and $\calC_2$ tells us that no single event $B \in \calC_2$ can provide us with information about $A$. The result above then implies that neither can any event in $\delta(\calC_2)$. In particular, taking complements and increasing countable unions cannot give us information that was available in a single event in $\calC_2$ to begin.

But if we are thinking of $\calC_2$ as information, then surely it makes sense to consider two different events $B_1, B_2 \in \calC_2$ simultaneously. After all, knowing whether $B_1$ and $B_2$ each have occurred we can conclude whether $B_1 \union B_2$ and $B_1 \intersect B_2$ have occurred as well. In other words, $\calC_2$ must be a set algebra if it is to model information. But then \cref{thm:Dynkin-independence} implies that $\calC_1$ and $\sigma(\calC_2)$ are independent: no information in $\sigma(\calC_2)$ can improve our knowledge of $A$ if no single event in the algebra $\calC_2$ can.

In some sense then, if $\calA$ is a set algebra in $\Omega$, the generated $\sigma$-algebra $\sigma(\calA)$ carries no more information than $\calA$. This picture is not quite complete, of course: If $\calA$ is unable to give us \emph{any information whatsoever} about some event $E$, then $\sigma(\calA)$ cannot either. But why should that mean that there is no increasing in information at all when passing from $\calA$ to $\sigma(\calA)$?

To further probe the interpretation of $\sigma$-algebras as information, we introduce the following terminology:

\begin{definition}
    Let $\calC$ be a collection of subsets of a set $\Omega$. We say that points $\omega, \omega' \in \Omega$ are \emph{$\calC$-equivalent} if, for every $A \in \calC$, they both lie in $A$ or $A^c$, i.e. if $\indicator{A}(\omega) = \indicator{A}(\omega')$. In this case we write $\omega \sim_\calC \omega'$.

    The relation $\sim_\calC$ induces a partition of $\Omega$ called the \emph{$\calC$-partition}.
\end{definition}
%
Another way to put this is that $\omega$ and $\omega'$ lie in all the same sets in $\calC$. Compare this with the \emph{topological indistinguishability} relation from point-set topology: Two points $x,y \in X$ in a topological space $X$ are called topologically indistinguishable if they have all the same (open) neighbourhoods.

\begin{lemma}
    Given points $\omega, \omega' \in \Omega$ we have $\omega \sim_\calC \omega'$ if and only if $\omega \sim_{\sigma(\calC)} \omega'$. In particular, the $\calC$- and $\sigma(\calC)$-partitions of $\Omega$ coincide.
\end{lemma}

\begin{proof}
    The latter clearly implies the former, so assume that $\omega \sim_\calC \omega'$. The collection of subsets $A \subseteq \Omega$ such that $\indicator{A}(\omega) = \indicator{A}(\omega')$ is clearly a $\sigma$-algebra, and it contains $\calC$ by assumption. But then it must also contain $\sigma(\calC)$, so $\omega \sim_{\sigma(\calC)} \omega'$.
\end{proof}
%
Now consider drawing a random element $\omega$ from $\Omega$. If an observer has the information $\calC$, i.e. they know whether or not $\omega \in A$ for all $A \in \calC$, then all they know is which $\calC$-equivalence class $\omega$ lies in. But by the lemma, this class is the same as the $\sigma(\calC)$-equivalence class of $\omega$, so again passing from $\calC$ to $\sigma(\calC)$ yields no new information.

% In general, the smaller a set $A$ is, the more information one gains by knowing that $\omega \in A$. Furthermore, the more sets one has at one's disposal, i.e. the larger $\calC$ is, the more information one has about $\omega$.

Thus it really does not seem like $\sigma$-algebras can do any more work than the algebras that generate them. And since we are comfortable with algebras carrying some kind of information, maybe $\sigma$-algebras do too.

To show that we cannot indiscriminately think of $\sigma$-algebras as information, we give an example of a case in which we seem to have both no information and a lot of information. This is Example~4.10 in \textcite{billingsley1995}.

\begin{example}
    Consider the probability space $([0,1], \calF, \lambda)$, where $\calF$ is the Borel algebra $\borel{[0,1]}$ and $\lambda$ is the Lebesgue measure restricted to $[0,1]$. Furthermore, let $\calG$ be the sub-$\sigma$-algebra of $\calF$ consisting of countable and cocountable sets. Then the measure of each element in $\calG$ is either $0$ or $1$, so $\calF$ and $\calG$ are independent. Given some event $E \in \calF$,
    %
    \begin{enumerate}[label=\normalfont(\alph*)]
        \item $\calF$ contains \emph{no} information about $E$, in the sense that $E$ is independent of $\calF$.
    \end{enumerate}
    %
    On the other hand, the $\calG$-equivalence classes are singletons, so
    %
    \begin{enumerate}[resume, label=\normalfont(\alph*)]
        \item $\calF$ contains \emph{all} the information about $E$, for given $\calF$ an observer knows precisely which $\omega$ was drawn, hence whether $E$ occurred or not.
    \end{enumerate}
    %
    These are in apparent contradiction, so it must not be the case that we can always interpret $\sigma$-algebras as information.

    Of course this example is rather artificial (indeed $\calG$ is not even countably generated, and $\lambda$ restricted to $\calG$ is also almost trivial), and it should not be seen as prohibiting the interpretation of $\sigma$-algebras as information entirely.
\end{example}


\section{Random variables}

\newcommand{\meas}[1]{\mathcal{M}(#1)}
\newcommand{\simplemeas}[1]{\mathcal{S\!M}(#1)}

There are various ways of motivating the definition of measurability of functions in general measure theory: In order to make possible integration, by analogy with continuous maps between topological spaces, or heuristically by appealing to an intuitive notion of measurability.

For random variables we do not have this luxury. Why should random variables be integrable? Why should random variables have anything to do with continuity? Hence we focus on the third point and try to explore what measurability means for random variables.

We begin by fixing terminology and notation: If $(X,\calE)$ and $(Y,\calF)$ are measure spaces, a map $f \colon X \to Y$ is said to be \emph{$(\calE,\calF)$-measurable} if $f\preim(B) \in \calE$ for all $B \in \calF$. Denote by $\meas{\calE}$ the space of functions $X \to \reals$ that are $(\calE,\borel{X})$-measurable. Recall that a function is called \emph{simple} if its image is finite. Denote the subspace of $\meas{\calE}$ consisting of simple measurable functions by $\simplemeas{\calE}$.

A \emph{random variable} on a probability space $(\Omega,\calF,\P)$ is a function in $\meas{\calF}$. Before studying general random variables, let us consider simple random variables. Measurability of a simple random variable $\rvar{X} \colon \Omega \to \reals$ means that
%
\begin{equation*}
    \{ \rvar{X} = x \}
        \defn \set{\omega \in \Omega}{\rvar{X}(\omega) = x}
        \in \calF
\end{equation*}
%
for all $x \in \reals$. Say that we have drawn some $\omega \in \Omega$. Given the information in $\calF$, can we determine the value of $\rvar{X}(\omega)$? If $\{\rvar{X} = x\} \in \calF$, then we know whether or not $\omega \in \{\rvar{X} = x\}$, i.e. whether $\rvar{X}(\omega) = x$. Thus if $\{\rvar{X} = x\}$ lies in $\calF$ for all $x \in \reals$ (indeed it is sufficient that this holds for all $x$ in the range of $\rvar{X}$), then we do have enough information to determine $\rvar{X}(\omega)$. In fact, since the range of $\rvar{X}$ is finite we only need to check the finitely many sets on the form $\{\rvar{X} = x\}$, as $x$ runs through the range of $\rvar{X}$. Of course this assumes that we have a list of the values that $\rvar{X}$ obtains. If this is not the case we can still approximate $x$ arbitrarily well in finitely many steps, as we will see below. Another way to put this is that the $\sigma$-algebra $\sigma(\rvar{X})$ generated by $\rvar{X}$ corresponds to knowledge of the value $\rvar{X}(\omega)$. [If we have $\calF$, how do we know which sets to use? If not, why look at simple random variables?]

Next we consider a general random variable $\rvar{X} \in \meas{\calF}$. By the same reasoning, for $\calF$ to be able to determine $\rvar{X}(\omega)$ given $\omega \in \Omega$ it must contain the sets $\{\rvar{X} = x\}$ for $x \in \rvar{X}(\Omega)$. But we claim that this is not sufficient for $\rvar{X}$ to be measurable:
%
\begin{example}
    Let $\calE$ be the countable, cocountable $\sigma$-algebra on $\reals$, and let $\rvar{X} \colon \reals \to \reals$ be the identity function. Then $\{\rvar{X} = x\} = \{x\} \in \calE$, but $\{\rvar{X} \in [0,\infty)\} = [0, \infty)$, which is neither countable nor cocountable. Hence $\rvar{X}$ has measurable fibres despite not being measurable.
\end{example}
%
Hence we need to come up with a different criterion. Instead of trying to determine the value of $\rvar{X}$ exactly, we attempt to approximate it. Suppose for definiteness that we have drawn an element $\omega \in \Omega$ such that $x = \rvar{X}(\omega) > 0$, and that we wish to determine $x$ within some error $\epsilon > 0$. First, check if $x$ lies in the interval $(-\infty,n)$ for $n \in \naturals$, starting with $n = 1$. After a finite number of checks we find that $x \in [n_0, n_0+1)$ for some $n_0 \in \naturals$. Next, keep dividing this interval in half by checking if $x \in (-\infty, a)$ for appropriate $a \in \rationals$, always keeping the part of the interval in which $x$ lies and discarding the other. Eventually we are left with an interval $[p,q)$ with $q-p < \epsilon$, and hence we obtain in finitely many steps an approximation $r \in \rationals$ of $x$ such that $\abs{x-r} < \epsilon$.

Notice that we only used intervals on the form $(-\infty, a)$ with $a \in \rationals$ during this process. Thus only knowledge of the events $\{\rvar{X} \in (-\infty, a)\}$ is necessary to determine $\rvar{X}(\omega)$, at least approximately. And since the sets $(-\infty, a)$ generate the Borel algebra $\borel{\reals}$, this requirement is the same as requiring $\rvar{X}$ to be $(\calF,\borel{\reals})$-measurable.


\section[Sub-sigma-algebras and conditional expectation][Sub-$\sigma$-algebras and conditional expectation]{Sub-$\sigma$-algebras and conditional expectation}

\newcommand{\condexp}[2]{\mathbb{E}[#1 \mid #2]}

If $(\Omega, \calF, \P)$ is a probability space, then we have seen that we, at least to some degree, can think of $\calF$ as the amount of information we have available. More precisely, given any event $A \in \calF$ we are able to decide whether $A$ has occurred or not. Furthermore, a map $\rvar{X} \colon \Omega \to \reals$ being $\calF$-measurable means that, given $\omega \in \Omega$, we are able to approximate $\rvar{X}(\omega)$ to arbitrary precision in finitely many steps.

But what happens if we do not have access to all the information in $\calF$, but only to the information in some sub-$\sigma$-algebra $\calB \subseteq \calF$? If $\rvar{X}$ is not $\calB$-measurable, then there is some $a \in \reals$ such that we cannot even tell whether $\rvar{X} < a$ or not. We wish to construct a random variable that in some sense is the best approximation of $\rvar{X}$, using only the information in $\calB$.

First let $B \in \calB$ be an event with $\P(B) > 0$. We define the \emph{conditional expectation of $\rvar{X}$ given $B$} by
%
\begin{equation*}
    \condexp{\rvar{X}}{B}
        = \frac{\mean{\rvar{X} \indicator{B}}}{\P(B)}
        = \frac{1}{\P(B)} \int_B \rvar{X} \dif\P.
\end{equation*}
%
If the mean $\mean{X}$ is the best approximation of $\rvar{X}$ given no further information, we interpret $\condexp{\rvar{X}}{B}$ as the best approximation of $\rvar{X}$ given that $B$ has occurred. Since we \emph{know} that $B$ has occurred there is nothing random about $\condexp{\rvar{X}}{B}$. The above of course requires that the mean of $\rvar{X}$ and $\rvar{X \indicator{B}}$ exist. To ensure this we will assume that $\rvar{X} \in \calL^1(\P)$.

Next let $\calB$ be the sub-$\sigma$-algebra of $\calF$ generated by $B$, i.e. $\calB = \{ \emptyset, B, B^c, \Omega \}$. We would then expect the conditional expectation of $\rvar{X}$ given $\calB$ to be the random variable $\condexp{\rvar{X}}{\calB}$ given by
%
\begin{equation}
    \label{eq:conditional-expectation-1}
    \condexp{\rvar{X}}{\calB}(\omega)
        =
        \begin{cases}
            \condexp{\rvar{X}}{B},   & \omega \in B, \\
            \condexp{\rvar{X}}{B^c}, & \omega \in B^c.
        \end{cases}
\end{equation}
%
That is, if we know that $\omega \in B$, then the best approximation of $\rvar{X}$ must be the conditional probability of $\rvar{X}$ given $B$, and similarly if $\omega \in B^c$. If more generally $\calB$ is finitely generated by events $B_1, \ldots, B_n$, then each $\omega \in \Omega$ lies in precisely one of $B_i$ and $B_i^c$ for $i = 1, \ldots, n$. That is, $\Omega$ can be partitioned into sets $B_1^* \intersect \cdots \intersect B_n^*$, where each $B_i^*$ is either $B_i$ or $B_i^*$. We would then have
%
\begin{equation*}
    \condexp{\rvar{X}}{\calB}(\omega)
        = \condexp{\rvar{X}}{ B_1^* \intersect \cdots \intersect B_n^* },
        \quad \text{for} \quad
        \omega \in B_1^* \intersect \cdots \intersect B_n^*.
\end{equation*}
%
Clearly this reduces to \cref{eq:conditional-expectation-1} when $n = 1$.

If $\calB$ is not finitely generated, then we cannot decompose $\Omega$ as above. It thus seems unlikely that we should be able to write down $\condexp{\rvar{X}}{\calB}$ explicitly in this case. Instead we will try to find properties that $\condexp{\rvar{X}}{\calB}$ must have for it to properly be called an approximation of $\rvar{X}$.

If nothing else, its mean should certainly agree with $\mean{\rvar{X}}$. Furthermore, for it to be a proper approximation of $\rvar{X}$ it should probably also resemble $\rvar{X}$ locally. But we should be careful here: If we zoom in too far and let $\condexp{\rvar{X}}{\calB}$ \emph{equal} $\rvar{X}$ at each point, then we have gotten nowhere.

Furthermore, it is an easy theorem (see e.g. Thorbjørnsen Sætning~10.2.1), whose proof we will not reproduce here, that integrable functions whose integrals on any measurable set agree are equal almost everywhere. In probabilistic terms, $\calF$-measurable random variables $\rvar{X}, \rvar{Y} \in \calL^1(\P)$ are equal almost surely in the case that $\condexp{\rvar{X}}{B} = \condexp{\rvar{Y}}{B}$ for all $B \in \calF$. But in our case we do not have access to every event in $\calF$, those those that lie in $\calB$. This motivates the following definition, which is easily seen to be a generalisation of the ones we have considered so far:

\begin{definition}[Conditional expectations]
    Let $(\Omega, \calF, \P)$ be a probability space, let $\calB$ be a sub-$\sigma$-algebra of $\calF$, and let $\rvar{X} \in \calL^1(\P)$ be a random variable.

    A \emph{conditional expectation of $\rvar{X}$ given $\calB$} is a random variable $\rvar{U}$ on $(\Omega, \calF, \P)$ such that
    %
    \begin{enumdef}
        \item $\rvar{U} \in \calL^1(\P)$,
        \item $\rvar{U}$ is $\calB$-measurable, and
        \item \label{enum:conditional-expectation-local-approx} $\condexp{\rvar{U}}{B} = \condexp{\rvar{X}}{B}$ for all $B \in \calB$ with $\P(B) > 0$.
    \end{enumdef}
\end{definition}
%
Notice that condition \subcref{enum:conditional-expectation-local-approx} is equivalent to the requirement that
%
\begin{equation*}
    \int_B \rvar{U} \dif\P
        = \int_B \rvar{X} \dif\P
\end{equation*}
%
for all $B \in \calB$ (not just those with positive measure).


\nocite{*}

\printbibliography


\end{document}