% Document setup
\documentclass[article, a4paper, 11pt, oneside]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{babel}

% Document info
\newcommand\doctitle{Conceptual foundations of probability theory}
\newcommand\docauthor{Danny NygÃ¥rd Hansen}

% Formatting and layout
\usepackage[autostyle]{csquotes}
\renewcommand{\mktextelp}{(\textellipsis\unkern)}
\usepackage[final]{microtype}
\usepackage{xcolor}
\frenchspacing
\usepackage{latex-sty/articlepagestyle}
\usepackage{latex-sty/articlesectionstyle}

% Fonts
\usepackage{amssymb}
\usepackage[largesmallcaps,partialup]{kpfonts}
\DeclareSymbolFontAlphabet{\mathrm}{operators} % https://tex.stackexchange.com/questions/40874/kpfonts-siunitx-and-math-alphabets
\linespread{1.06}
% \let\mathfrak\undefined
% \usepackage{eufrak}
\DeclareMathAlphabet\mathfrak{U}{euf}{m}{n}
\SetMathAlphabet\mathfrak{bold}{U}{euf}{b}{n}
% https://tex.stackexchange.com/questions/13815/kpfonts-with-eufrak
\usepackage{inconsolata}

% Hyperlinks
\usepackage{hyperref}
\definecolor{linkcolor}{HTML}{4f4fa3}
\hypersetup{%
	pdftitle=\doctitle,
	pdfauthor=\docauthor,
	colorlinks,
	linkcolor=linkcolor,
	citecolor=linkcolor,
	urlcolor=linkcolor,
	bookmarksnumbered=true
}

% Equation numbering
\numberwithin{equation}{chapter}

% Footnotes
\footmarkstyle{\textsuperscript{#1}\hspace{0.25em}}

% Mathematics
\usepackage{latex-sty/basicmathcommands}
\usepackage{latex-sty/framedtheorems}
\usepackage{latex-sty/probabilitycommands}
\usepackage{tikz-cd}
\tikzcdset{arrow style=math font} % https://tex.stackexchange.com/questions/300352/equalities-look-broken-with-tikz-cd-and-math-font
\usetikzlibrary{babel}

% Lists
\usepackage{enumitem}
\setenumerate[0]{label=\normalfont(\arabic*)}

% Bibliography
\usepackage[backend=biber, style=authoryear, maxcitenames=2, useprefix]{biblatex}
\addbibresource{references.bib}

% Title
\title{\doctitle}
\author{\docauthor}


\newcommand{\calN}{\mathcal{N}}
\DeclarePairedDelimiter{\nhoodfilteraux}{(}{)}
% \newcommand{\nhoodfilter}[1]{\calN\nhoodfilteraux{#1}}
\newcommand{\nhoodfilter}[1]{\calN_{#1}}


\newcommand{\calU}{\mathcal{U}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calW}{\mathcal{W}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calJ}{\mathcal{J}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calL}{\mathcal{L}}

\newcommand{\borel}[1]{\calB(#1)}
\DeclareMathOperator{\supp}{supp}

% Probability
\let\oldP\P
\renewcommand{\P}{\mathbb{P}}

\renewcommand{\mean}[2][]{\mathbb{E}\meanaux[#1]{#2}}

\DeclarePairedDelimiterX{\condexpaux}[2]{[}{]}{#1\;\delimsize\vert\;#2}
\newcommand{\condexp}[3][]{{\mathbb{E}\condexpaux[#1]{#2}{#3}}}
% \newcommand{\condexp}[2]{\mathbb{E}[#1 \mid #2]}

% Use DeclarePairedDelimiter to get delimiter sizes
% \newcommand{\condP}[2]{\P(#1 \mid #2)}
\DeclarePairedDelimiterX{\condPaux}[2]{(}{)}{#1\;\delimsize\vert\;#2}
\newcommand{\condP}[3][]{{\P\condPaux[#1]{#2}{#3}}}

% Add to basicmathcommands.sty
\renewcommand{\symdiff}{\mathbin{\triangle}}

\newcommand{\keyword}[1]{{\itshape\bfseries #1}}


\begin{document}

\maketitle

\chapter{Introduction}

\begin{theorem}[The strong law of large numbers]
    \label{thm:strong-LLN}
    Let $(\rvar{X}_n)_{n\in\naturals}$ be a sequence of i.i.d. integrable random variables on a probability space $(\Omega,\calF,\P)$. Then
    %
    \begin{equation*}
        \lim_{n\to\infty} \frac{1}{n} \sum_{i=1}^n \rvar{X}_i(\omega)
            = \mean{\rvar{X}_1}
    \end{equation*}
    %
    for $\P$-almost all $\omega \in \Omega$.
\end{theorem}

\begin{proof}
    \textcite[Theorem~12.1]{bauer1995} or \textcite[Theorem~22.1]{billingsley1995}.
\end{proof}

In the usual measure-theoretical formulation of probability theory, the following result is a corollary of the Law of Large Numbers:

\begin{theorem}[The frequency interpretation of probability]
    Let $\rvar{X}, \rvar{X}_1, \rvar{X}_2, \ldots$ be i.i.d. real-valued random variables on a probability space $(\Omega, \calF, \P)$. For every $B \in \borel{\reals}$ we have
    %
    \begin{equation*}
        \P(\rvar{X} \in B)
            = \lim_{n\to\infty} \frac{
                \card{ \set{j \in \{1, \ldots, n\} }{ \rvar{X}_j(\omega) \in B } }
            }{
                n
            }
    \end{equation*}
    %
    for $\P$-almost all $\omega \in \Omega$.
\end{theorem}

\begin{proof}
    This follows directly by applying \cref{thm:strong-LLN} to the sequence $(\indicator{B}(\rvar{X}_n))_{n\in\naturals}$.
\end{proof}
%
That is, given a sequence $(\rvar{X}_n)_{n\in\naturals}$, and one extra $\rvar{X}$, of i.i.d. random variables, the probability that $\rvar{X}$ lies in some Borel set $B$ can be thought of as the proportion of the $\rvar{X}_n$ that lie in $B$, as $n$ tends to infinity. In other words, probability is a measure of the \emph{frequency} with which an outcome of a random experiment obtains, if we repeat the experiment many times.

Whether or not this is the correct interpretation of probability as it occurs in the natural world we will not discuss here. Nonetheless the above result is an uncontroversial consequence of the theory, and it certainly aligns with our intuitive understanding of probability.

In this note we turn this result on its head and attempt to use it to motivate the formalisation of probability theory in terms of measure spaces. As we shall see, this is not entirely successful and will require some leaps that are not entirely justified by our conceptual grasp of probability.


\chapter{Preliminaries}


\section{Boolean algebras}

We begin by reviewing some of the purely algebraic properties of Boolean algebras.

\begin{definition}[Boolean algebras]
    \label{def:Boolean-algebra}
    A \keyword{Boolean algebra} is a structure $\langle B; \join, \meet, ', 0, 1 \rangle$ such that
    %
    \begin{enumdef}
        \item $\langle B; \join, \meet \rangle$ is a distributive lattice,
        \item $0$ and $1$ are elements of $B$ such that $x \join 0 = x$ and $x \meet 1 = x$ for all $x \in B$, and
        \item $'$ is a unary operation such that $x \join x' = 1$ and $x \meet x' = 0$ for all $x \in B$.
    \end{enumdef}
\end{definition}
%
The binary operations $\join$ and $\meet$ are called \keyword{join} and \keyword{meet}, respectively. For $x \in B$ the element $x'$ is called the \keyword{complement} of $x$. In a general bounded lattice $L$, an element $y \in L$ such that $x \join y = 1$ and $x \meet y = 0$ is called a complement of $x \in L$. If $L$ is distributive, complements are unique. Recall also that the lattice structure on $B$ induces a partial order $\leq$ such that $x \leq y$ if and only if $x \join y = y$ for $x,y \in B$.

If $x \meet y = 0$, then $x$ and $y$ are said to be \keyword{disjoint}. A collection $\{x_i\}_{i \in I}$ of elements in $B$ is called \keyword{pairwise disjoint} if $x_i$ and $x_j$ are disjoint for any choice of indices $i \neq j$.

Let $B$ be a Boolean algebra. For $x,y \in B$ we define the \keyword{symmetric difference} between $x$ and $y$ by
%
\begin{equation*}
    x \symdiff y
        = (x \meet y') \join (y \meet x').
\end{equation*}
%
If $x \symdiff y = 0$, then it is easy to show that $x = y$.

Before proceeding we note the following technical result that we shall need later:

\begin{lemma}
    \label{thm:refine_join}
    Let $\langle B; \join, \meet, ', 0, 1 \rangle$ be a Boolean algebra. Let $\{x_i\}_{i \in I}$ be a collection of pairwise disjoint elements in $B$. If $\bigjoin_{i \in I} x_i \in B$, then $\bigjoin_{i \in J} x_i \in B$ for any cofinite\footnotemark{} $J \subseteq I$.
\end{lemma}

\begin{proof}\footnotetext{Recall that a subset $J$ of a set $I$ is called \emph{cofinite} if the complement $I \setminus J$ is finite.}
    Let $(x_i)_{i \in I}$ be such a collection of elements, and let $J \subseteq I$ be cofinite. It suffices to prove the lemma in the case $I \setminus J = \{i_0\}$, since the general case then follows by induction. We claim that
    %
    \begin{equation}
        \label{eq:Boolean-algebra-lemma}
        \bigjoin_{i \in J} x_i
            = x_{i_0}' \meet \bigjoin_{i \in I} x_i,
    \end{equation}
    %
    in which case the claim would follow. Let $j \in J$ and notice that, since $x_{i_0} \meet x_j = 0$,
    %
    \begin{equation*}
        x_{i_0}' \meet x_j
            = (x_{i_0}' \meet x_j) \join (x_{i_0} \meet x_j)
            = (x_{i_0}' \join x_{i_0}) \meet x_j
            = 1 \meet x_j
            = x_j,
    \end{equation*}
    %
    i.e. $x_j \leq x_{i_0}'$. Now because also $x_j \leq \bigjoin_{i \in I} x_i$ we get
    %
    \begin{equation*}
        x_j
            \leq x_{i_0}' \meet \bigjoin_{i \in I} x_i.
    \end{equation*}
    %
    Since $j \in J$ was arbitrary, the inequality $\leq$ in \cref{eq:Boolean-algebra-lemma} follows. Conversely, suppose that $s \in B$ is such that $x_j \leq s$ for all $j \in J$. Then $x_i \leq x_{i_0} \join s$ for all $i \in I$, so
    %
    \begin{equation*}
        \bigjoin_{i \in I} x_i
            \leq x_{i_0} \join s.
    \end{equation*}
    %
    It follows that
    %
    \begin{equation*}
        x_{i_0}' \meet \bigjoin_{i \in I} x_i
            \leq x_{i_0}' \meet (x_{i_0} \join s)
            = 0 \join (x_{i_0} \meet s)
            \leq s,
    \end{equation*}
    %
    which implies the inequality $\geq$ in \cref{eq:Boolean-algebra-lemma}.
\end{proof}



\section{Abstract measure spaces}

\begin{definition}[Generalised abstract measure spaces]
    A \keyword{measure} on a Boolean algebra $B$ is a map $\mu \colon B \to [0,\infty)$ such that
    %
    \begin{equation}
        \label{eq:finite-additivity}
        \mu(x \join y)
            = \mu(x) + \mu(y)
    \end{equation}
    %
    for all disjoint $x,y \in B$. If $\mu$ is a measure on a Boolean algebra $B$, then we call the pair $(B,\mu)$ a \keyword{generalised abstract measure space}. If $x \neq 0$ implies that $\mu(x) > 0$, then $\mu$ is called \keyword{positive definite}. If $\mu(1) = 1$, then we call $\mu$ a \keyword{probability measure}.
\end{definition}
%
It is clear that $\mu(\emptyset) = 0$ and that $\mu$ is increasing. It follows that $\mu(x) \leq \mu(1)$ for all $x \in B$. Notice that we require that $\mu$ is finite, but this is no restriction since we are ultimately interested in the case where $\mu$ is a probability measure. % I don't think we need to assume finiteness. Get back to this. -- We need it for the metric, but we can just replace the (possibly infinite) metric by an equivalent finite one.

The property \cref{eq:finite-additivity} is called \keyword{(finite) additivity} of $\mu$, since an easy induction argument extends it to all finite joins. We will later define more restrictive structures and measures upon them, hence the adjective \enquote{generalised}.

\begin{proposition}[Boole's inequality]
    Let $(B,\mu)$ be a generalised abstract measure space. Then for any $x,y \in B$ we have
    %
    \begin{equation*}
        \mu(x \join y)
            \leq \mu(x) + \mu(y).
    \end{equation*}
\end{proposition}
%
Similar to \cref{eq:finite-additivity}, Boole's inequality may be extended to all finite joins by induction.

\begin{proof}
    Notice that
    %
    \begin{equation*}
        (x \meet y') \meet y = x \meet (y' \meet y) = 0,
    \end{equation*}
    %
    so $x \meet y'$and $y$ are disjoint, and that
    %
    \begin{equation*}
        (x \meet y') \join y = (x \join y) \meet (y' \join y) = x \join y.
    \end{equation*}
    %
    It follows by additivity of $\mu$ that
    %
    \begin{equation*}
        \mu(x \join y)
            = \mu \bigl( (x \meet y') \join y \bigr)
            = \mu(x \meet y') + \mu(y)
            \leq \mu(x) + \mu(y),
    \end{equation*}
    %
    as desired.
\end{proof}


\begin{definition}[Metric Boolean algebras]
    A \keyword{pseudometric Boolean algebra} is a tuple $(B,\rho)$, where $B$ is a Boolean algebra and $\rho$ is a pseudometric on $B$ such that the maps $x \mapsto x'$, $(x,y) \mapsto x \join y$, and $(x,y) \mapsto x \meet y$ are continuous.

    If $\rho$ is a metric, then $(B,\rho)$ is called a \keyword{metric Boolean algebra}.
\end{definition}

Next we equip generalised abstract measure spaces with a canonical pseudometric. If $(B,\mu)$ is a generalised abstract measure space, define a map $\rho_\mu \colon B \times B \to [0,\infty)$ by
%
\begin{equation}
    \label{eq:Boolean-metric}
    \rho_\mu(x,y)
        = \mu(x \symdiff y)
\end{equation}
%
for $x,y \in B$. The next proposition shows that $\rho_\mu$ is in fact a pseudometric. We will always equip a generalised abstract measure space with this pseudometric.

\begin{proposition}
    Given a generalised abstract measure space $(B,\mu)$, the map $\rho_\mu$ defined in \cref{eq:Boolean-metric} makes $(B,\rho_\mu)$ into a pseudometric Boolean algebra. Furthermore, $\rho_\mu$ is a metric if and only if $\mu$ is positive definite.
\end{proposition}


\begin{proof}
\begin{proofsec}
    \item[$\rho_\mu$ is a pseudometric]
    We only need to prove the triangle inequality. To this end, let $x,y,z \in B$ and notice that
    %
    \begin{align*}
        x \meet z'
            &= (x \meet z) \meet (y' \join y) \\
            &= (x \meet z' \meet y') \join (x \meet z' \meet y) \\
            &\leq (x \meet y') \join (y \meet z').
    \end{align*}
    %
    Similarly we have $z \meet x' \leq (z \meet y') \join (y \meet x')$. It follows that
    %
    \begin{align*}
        x \symdiff z
            &= (x \meet z') \join (z \meet x') \\
            &\leq (x \meet y') \join (y \meet x') \join
                  (y \meet z') \join (z \meet y') \\
            &= (x \symdiff y) \join (y \symdiff z).
    \end{align*}
    %
    Now Boole's inequality implies that
    %
    \begin{equation*}
        \rho_\mu(x,z)
            = \mu(x \symdiff y)
            \leq \mu(x \symdiff y) + \mu(y \symdiff z)
            = \rho_\mu(x,y) + \rho_\mu(y,z),
    \end{equation*}
    %
    as desired.

    \item[Continuity of lattice operations]
    Let $x \in B$, and let $(x_n)_{n\in\naturals}$ be a sequence in $B$ that converges to $x$. Notice that $x_n' \symdiff x' = x_n \symdiff x$, so $\rho_\mu(x_n',x') = \rho_\mu(x_n,x)$. Hence the complementation map $x \mapsto x'$ is continuous.

    Let further $(y_n)_{n\in\naturals}$ be a sequence converging to a point $y \in B$. A short calculation shows that
    %
    \begin{align*}
        (x_n \join y_n) \symdiff (x \join y)
            &= (x_n \meet x' \meet y') \join
               (y_n \meet x' \meet y') \join
               (x \meet x_n' \meet y_n') \join
               (y \meet x_n' \meet y_n') \\
            &\leq (x_n \meet x') \join
            (x \meet x_n') \join
            (y_n \meet y') \join
            (y \meet y_n') \\
            &= (x_n \symdiff x) \join (y_n \symdiff y).
    \end{align*}
    %
    Thus Boole's inequality shows that
    %
    \begin{equation}
        \label{eq:join-continuous}
        \rho_\mu(x_n \join y_n, x \join y)
            \leq \rho_\mu(x_n,x) + \rho_\mu(y_n,y),
    \end{equation}
    %
    which implies continuity of the join map $(x,y) \mapsto x \join y$.

    Finally, continuity of the meet map $(x,y) \mapsto x \meet y$ follows since
    %
    \begin{equation*}
        x \meet y
            = \bigl( x' \join y' \bigr)',
    \end{equation*}
    %
    so it is a composition of continuous functions.

    \item[Positive definiteness]
    The last claim follows directly from the fact that $x \symdiff y = 0$ if and only if $x = y$, for all $x,y \in B$.
\end{proofsec}
\end{proof}

\begin{remark}
    \label{rem:convergence-of-measure}
    Notice that the measure $\mu$ can be written in terms of $\rho_\mu$, since $\mu(x) = \rho_\mu(x,0)$. Furthermore, since (pseudo)metrics are continuous, it follows that $\mu(x_n) \to \mu(x)$ whenever $x_n \to x$ in $B$.
\end{remark}


It is well-known that any (pseudo)metric space has a completion, i.e. can be isometrically embedded as a dense subset of a complete (pseudo)metric space. See for instance Corollary~24.5 in \textcite{willard}. A natural question is then: If $(B,\rho)$ is a (pseudo)metric Boolean algebra with metric completion $(\altoverline{B}, \altoverline{\rho})$, does $\altoverline{B}$ also carry the structure of a Boolean algebra?

This is indeed the case, and we sketch the construction: Let $x,y \in \altoverline{B}$, and let $(x_n)$ and $(y_n)$ be sequences in $B$ that converge to $x$ and $y$, respectively. Then these are Cauchy sequences in $B$, and the calculation leading to \cref{eq:join-continuous} shows that $(x_n \join y_n)$ is also a Cauchy sequence. Thus it converges to some element of $\altoverline{B}$. Denote it $x \join y$. We define $x'$ and $x \meet y$ similarly. It is easy to check that these operations satisfy the conditions in \cref{def:Boolean-algebra}. Furthermore, the completion $\altoverline{\rho}$ of the pseudometric $\rho$ makes $(\altoverline{B}, \altoverline{\rho})$ into a pseudometric Boolean algebra.

This takes care of the metric structure. The next proposition shows that we can also extend the measure on a generalised abstract measure space to its completion.

\begin{proposition}
    Let $(B,\mu)$ be a generalised abstract measure space, and let $(\altoverline{B}, \altoverline{\rho}_\mu)$ be the completion of $(B,\rho_\mu)$. Define a map $\altoverline{\mu} \colon \altoverline{B} \to [0,\infty)$ by
    %
    \begin{equation}
        \label{eq:mu-overline-def}
        \altoverline{\mu}(x) = \lim_{n\to\infty} \mu(x_n),
    \end{equation}
    %
    where $(x_n)_{n\in\naturals}$ is any sequence in $B$ that converges to $x$. Then $\altoverline{\mu}$ is a well-defined measure on $\altoverline{B}$. The generalised abstract measure space $(\altoverline{B}, \altoverline{\mu})$ is called the \keyword{completion} of $(B,\mu)$.
\end{proposition}

\begin{proof}
    First notice that for any $x \in \altoverline{B}$ there does in fact exist a sequence in $B$ converging to $x$. If $(x_n)_{n\in\naturals}$ is such a sequence, it is a Cauchy sequence in $B$, and the reverse triangle inequality shows that $(\mu(x_n))$ is a Cauchy sequence in $\reals$, hence convergent. Thus the limit on the right-hand side of \eqref{eq:mu-overline-def} exists.
    
    Now let $(y_n)$ be another sequence in $B$ that approximates $x$. Another application of the reverse triangle inequality then shows that
    %
    \begin{equation*}
        \abs{ \mu(x_n) - \mu(y_n) }
            \leq \rho_\mu(x_n,y_n)
            \leq \rho_\mu(x_n,x) + \rho_\mu(y_n,x)
            \to 0.
    \end{equation*}
    %
    Hence $\mu(x_n)$ and $\mu(y_n)$ converge to the same value, and thus $\altoverline{\mu}$ is well-defined.

    Next we show that $\altoverline{\mu}$ is finitely additive. Let $x,y \in \altoverline{B}$ with $x \meet y = 0$ and choose approximating sequences $(x_n)$ and $(y_n)$ in $B$. Then
    %
    \begin{equation*}
        (x_n \join y_n) \meet (x_n \meet y_n)'
            = \bigl( x_n \meet (x_n \meet y_n)' \bigr)
              \join \bigl( y_n \meet (x_n \meet y_n)' \bigr)
    \end{equation*}
    %
    is the join of disjoint elements of $B$, so
    %
    \begin{equation*}
        \mu \bigl( (x_n \join y_n) \meet (x_n \meet y_n)' \bigr)
            = \mu \bigl( x_n \meet (x_n \meet y_n)' \bigr)
              + \mu \bigl( y_n \meet (x_n \meet y_n)' \bigr).
    \end{equation*}
    %
    By continuity of the lattice operations we have $(x_n \meet y_n)' \to 1$, so the three elements given as arguments to $\mu$ above are elements in approximating sequences for $x \join y$, $x$ and $y$ respectively. By definition of $\altoverline{\mu}$ it follows that
    %
    \begin{equation*}
        \altoverline{\mu}(x \join y)
            = \altoverline{\mu}(x) + \altoverline{\mu}(y)
    \end{equation*}
    %
    as desired.
\end{proof}


\begin{lemma}
    \label{thm:monotonic-sequence-Cauchy}
    Let $(B,\mu)$ be a generalised abstract measure space. Every monotonic sequence in $B$ is a Cauchy sequence.
\end{lemma}
% If we allow $\mu$ to be infinite, we probably need to assume that it is "bounded" as well.

\begin{proof}
    Let $(x_n)_{n\in\naturals}$ be an increasing sequence in $B$. Then since $x_n \leq 1$ we also have $\mu(x_n) \leq \mu(1) < \infty$ for all $n \in \naturals$. Thus the sequence $(\mu(x_n))$ is a bounded increasing sequence in $\setR$, hence it converges to some $\alpha \geq 0$. For $\epsilon > 0$ there is an $N \in \naturals$ such that $\mu(x_n) \in (\alpha - \epsilon, \alpha]$ for all $n \geq N$.
    
    If then $m,n \geq N$ with $m \leq n$, then $x_m \leq x_n$ and so
    %
    \begin{equation*}
        x_m \meet x_n'
            \leq x_n \meet x_n'
            = 0.
    \end{equation*}
    %
    Hence $x_m \symdiff x_n = x_n \meet x_m'$, so it follows that
    %
    \begin{equation*}
        \rho_\mu(x_m,x_n)
            = \mu(x_n \meet x_m')
            = \mu(x_n) - \mu(x_m)
            < \epsilon.
    \end{equation*}
    %
    Thus $(x_n)$ is indeed a Cauchy sequence. The case where $(x_n)$ is decreasing is similar.
\end{proof}


\begin{proposition}
    \label{thm:existence-of-joins}
    Let $(B,\mu)$ be a generalised abstract measure space with completion $(\altoverline{B}, \altoverline{\mu})$. Then any sequence $(x_n)_{n\in\naturals}$ in $B$ has a join $s$ in $\altoverline{B}$, and
    %
    \begin{equation*}
        \bigjoin_{i \leq n} x_i
            \to s
    \end{equation*}
    %
    as $n \to \infty$. Similarly for meets.
\end{proposition}

\begin{proof}
    The sequence $( \bigjoin_{i \leq n} x_i )_{n\in\naturals}$ is increasing, so by \cref{thm:monotonic-sequence-Cauchy} it has a limit $s \in \altoverline{B}$. We show that $s$ is the join of $(x_n)$. For $k \in \naturals$ and $n \geq k$ we have $x_k \leq \bigjoin_{i \leq n} x_i$, i.e.
    %
    \begin{equation*}
        x_k \join \bigjoin_{i \leq n} x_i
            = \bigjoin_{i \leq n} x_i.
    \end{equation*}
    %
    Taking the limit as $n \to \infty$, continuity of (binary) joins implies that $x_k \join s = s$ in $\altoverline B$, or $x_k \leq s$. Thus $s$ is an upper bound of the sequence $(x_n)$.

    On the other hand, if $t \in \altoverline{B}$ is an upper bound of $(x_n)$, then $x_k \leq t$. We have just seen that taking limits preserves inequalities, so this implies that $s \leq t$, and hence $s$ is the least upper bound.

    The corresponding result for meets follows similarly, or from the fact that complementation is continuous.
\end{proof}


\section[Abstract sigma-algebras and continuity][Abstract $\sigma$-algebras and continuity]{Abstract $\sigma$-algebras and continuity}

\begin{definition}[Axiom of continuity]
    A generalised abstract measure space $(B, \mu)$ is said to satisfy the \keyword{axiom of continuity} if it has the following property: If $(x_n)_{n\in\naturals}$ is a decreasing sequence of elements in $B$ such that $\bigmeet_{n\in\naturals} x_n$ exists and equals $0$, then $\lim_{n\to\infty} \mu(x_n) = 0$.
\end{definition}
%
This is an analogue of the continuity (from above) of ordinary countably additive measures on concrete $\sigma$-algebras.


\begin{lemma}
    \label{thm:positive-definite-implies-continuous}
    Let $(B, \mu)$ be a generalised abstract measure space with $\mu$ positive definite. Then $(B,\mu)$ satisfies the axiom of continuity.
\end{lemma}
%
As far as I know, the assumption that $\mu$ be positive definite is necessary for this to hold in general.

\begin{proof}
    Let $(x_n)_{n\in\naturals}$ be a decreasing sequence in $B$ such that $\bigmeet_{n\in\naturals} x_n = 0$. A similar argument to the one in the proof of \cref{thm:existence-of-joins} shows that $x_n$ converges to $0$ as $n \to \infty$ in $B$, hence also in the completion $\altoverline B$. Furthermore, since the sequence is decreasing we have $x_n = \bigmeet_{i \leq n} x_i$, so \cref{thm:existence-of-joins} also implies that the sequence converges to its meet $s$ in $\altoverline B$. But since $\mu$ is positive definite, limits in $\altoverline B$ are unique, so $s = 0$. By \cref{rem:convergence-of-measure}, $\mu(x_n)$ converges to $\mu(0) = 0$, proving the claim.
\end{proof}


\begin{proposition}[The generalised addition theorem]
    \label{thm:generalised-addition}
    Let $(B, \mu)$ be a generalised abstract measure space satisfying the axiom of continuity. If $(x_n)_{n\in\naturals}$ is a sequence of pairwise disjoint elements in $B$ such that $x = \bigjoin_{n\in\naturals} x_n \in B$, then
    %
    \begin{equation*}
        \mu(x)
            = \sum_{n=1}^\infty \mu(x_n).
    \end{equation*}
\end{proposition}

\begin{proof}
    By \cref{thm:refine_join}, $r_n = \bigjoin_{i > n} x_i$ exists in $B$, and we claim that $\bigmeet_{n\in\naturals} r_n = 0$. Let $t \in B$ be a lower bound of $r_n$ for $n \in \naturals$. Then for $n \in \naturals$ we have
    %
    \begin{equation*}
        t \join \bigjoin_{i > n} x_i
            = \bigjoin_{i > n} x_i,
    \end{equation*}
    %
    and taking the meet of each side with $x_n'$ yields $t \meet x_n' = 0$. Hence $\bigjoin_{n\in\naturals} t \meet x_n' = 0$. Now notice that $\bigmeet_{n\in\naturals} x_n = 0$, since $x_n \meet x_m = 0$ when $n \neq m$. It follows by taking complements that $\bigjoin_{n\in\naturals} x_n' = 1$, and so
    %
    \begin{equation*}
        t
            = t \meet \bigjoin_{n\in\naturals} x_n'
            = \bigjoin_{n\in\naturals} t \meet x_n'
            = 0.
    \end{equation*}
    %
    Thus $\bigmeet_{n\in\naturals} r_n = 0$ as claimed. It now follows from finite additivity of $\mu$ and the axiom of continuity that
    %
    \begin{equation*}
        \mu(x)
            = \sum_{i=1}^n \mu(x_i) + \mu(r_n)
            \to \sum_{i=1}^\infty \mu(x_i)
    \end{equation*}
    %
    as $n \to \infty$ as desired.
\end{proof}


\begin{proposition}[Boole's inequality]
    Let $(B,\mu)$ be a generalised abstract measure space, and let $(x_n)_{n\in\naturals}$ be a sequence in $B$. If $x = \bigjoin_{n\in\naturals} x_n \in B$, then
    %
    \begin{equation*}
        \mu(x)
            \leq \sum_{n=1}^\infty \mu(x_n).
    \end{equation*}
\end{proposition}% Is this correct??? Because it doesn't necessarily converge to its join in B but in the completion of B!

\begin{proof}
    First notice that, by the finite Boole equality,
    %
    \begin{equation*}
        \mu \Bigl( \bigjoin_{i \leq n} x_i \Bigr)
            \leq \sum_{i=1}^n \mu(x_i)
            \leq \sum_{i=1}^\infty \mu(x_i)
    \end{equation*}
    %
    for all $n \in \naturals$. By \cref{thm:existence-of-joins}, $\bigjoin_{i \leq n} x_i \to x$ as $n \to \infty$, so $\mu( \bigjoin_{i \leq n} x_i ) \to \mu(x)$. The claim follows
\end{proof}


Of course, the join of a sequence of elements may not exist. In the case where we are ensured the existence of countable joins we use the following terminology:

\begin{definition}[Abstract $\sigma$-algebra]
    \label{def:abstract-sigma-algebra}
    An \keyword{abstract $\sigma$-algebra} is a Boolean algebra $B$ with countable joins. That is, if $(x_n)_{n\in\naturals}$ is a sequence of elements in $B$, then their join $\bigjoin_{n\in\naturals} x_n$ exists.
\end{definition}
%
If the join $\bigjoin_{n\in\naturals} x_n$ exists, then it follows by taking complements that the meet $\bigmeet_{n\in\naturals} x_n'$ also exists. Hence an abstract $\sigma$-algebra also has countable meets. In the context of abstract measure spaces we obtain the following:

\begin{definition}[Abstract measure spaces]
    \label{def:abstract-measure-space}
    An \keyword{abstract measure space} is a generalised abstract measure space $(B,\mu)$ that satisfies the axiom of continuity, and where $B$ is an abstract $\sigma$-algebra.
\end{definition}

\begin{lemma}
    \label{thm:completion-gives-countable-additivity}
    If $(B,\mu)$ is a generalised abstract measure space with $\mu$ positive definite, then the completion $(\altoverline{B},\altoverline{\mu})$ is an abstract measure space.
\end{lemma}

\begin{proof}
    Since the completion of a metric space is a metric space, $\altoverline{\mu}$ is positive definite, so \cref{thm:positive-definite-implies-continuous} implies that $(\altoverline{B},\altoverline{\mu})$ satisfies the axiom of continuity.

    On the other hand, \cref{thm:existence-of-joins} implies that every sequence in $\altoverline{B}$ has a join, so $\altoverline{B}$ is an abstract $\sigma$-algebra.
\end{proof}



\chapter{The algebra of probability spaces}

\section{Motivation}

\newcommand{\compl}[1]{\altoverline{#1}}
\renewcommand{\P}{P}

If the probability of an event is supposed to be a measure of how often this event occurs on repetitions of the random experiment in question, then it seems reasonable to assume that we are, at least in principle, able to distinguish when the event does and does not obtain. For example, after rolling a six-sided die the state of affairs \enquote{the result of the die roll is three} is an event, since we can determine the outcome of the roll just by looking at the die. To take another example, after throwing a ball the state of affairs \enquote{the ball was thrown further than 50 metres} is also an event: That is, we can determine whether or not the length of the throw was strictly greater than 50 metres.

One might take a different view: Say that one grants that it is possible to \emph{affirm} that the length of the throw, measured in metres, lies in the interval $(50,\infty)$. If the length $L$ in metres does in fact lie in the above interval, we can simply use a ruler whose subdivisions are smaller than $L - 50$ in metres. Still one might disagree that it is possible to \emph{refute} that $L \in (50, \infty)$. For if $L$ is exactly $50$ metres, then since any measurement of $L$ carries some error, it is in practice impossible to determine whether $L$ is $50$ (or slightly smaller), or whether it is slightly larger than $50$. We will not pursue this line further but refer the reader to \textcite{vickers1989} for more on this \keyword{logic of affirmative assertions}.

To be precise, after performing the relevant random experiment, we will assume that we are always able to decide whether or not the event has occurred or not. In particular, if $E$ is an event, then the state of affairs \textquote{$E$ does not obtain} is also an event, denoted $\compl{E}$: If $E$ obtains, then $\compl{E}$ does not. And conversely, if $E$ does not obtain, then $\compl{E}$ does obtain. We call $\compl{E}$ the \keyword{complement of} or the \keyword{complementary event to $E$}.\footnote{In contrast, in the logic of affirmative assertions we do not allow complementation (i.e. negation). Hence it may not be surprising that this logic ends up being closely tied to topology, the relevant analogy being that the complement of an open set need not be open.} Evidently, the complement of a complement is just the event we started with.

Next consider two events $E_1$ and $E_2$. Since we are able to decide whether each of them have obtained, the same is true for the event \textquote{both $E_1$ and $E_2$ have obtained} and the event \textquote{at least one of $E_1$ and $E_2$ has obtained}. The first is called the \keyword{conjunction} of $E_1$ and $E_2$ and is denoted $E_1 \meet E_2$, and the second is the \keyword{disjunction} $E_1 \join E_2$ of $E_1$ and $E_2$.

Finally it seems natural to allow an \textquote{impossible event} $0$ which never occurs, as well as a \textquote{sure event} $1$ that always occurs. Clearly $0$ and $1$ are each other's complements. If $E_1$ and $E_2$ are events with $E_1 \meet E_2 = 0$, then this manifestly means that $E_1$ and $E_2$ cannot obtain simultaneously: The two events are \keyword{incompatible}.

We collect all the relevant events in a set $\calF$ and postulate that the structure $\langle \calF; \join, \meet, \compl{\,\cdot\,\vphantom{e}}, 0, 1 \rangle$ is a Boolean algebra. We leave it to the reader to reflect on the reasonability of this assumption. This leads naturally to the following definition:

\begin{definition}[Generalised abstract probability spaces]
    A \keyword{generalised abstract probability space} is a generalised abstract measure space $(\calF, \P)$, where $\P$ is a positive definite probability measure.
\end{definition}
%
The partial order $\leq$ induced by the lattice structure then has the interpretation that if $E \leq F$, then $E$ implies $F$. For recall that this means that $E \join F = F$, i.e. if $F$ has already occurred then no information is gained by observing that $E$ has also occurred. Conversely, if $F$ has \emph{not} occurred, then $E$ is impossible.

We have justified every part of this definition except for the positive definiteness of $\P$. In the usual measure theoretical formulation of probability theory, there may (and often do) exist events that are not empty but still have probability zero. Kolmogorov had the following to say in critique of this approach:
%
\blockquote[\cite{kolmogorov1995}]{%
    \textins*{W}e are forced to give up the principle, formulated in numerous classical works in probability theory, according to which an event of probability zero is absolutely impossible. More precisely, one must allow that an event of positive probability can be decomposed into a (possible continuous) infinity of variants of which each has probability
    zero.%
}
%
Furthermore, this also has the technical benefit that the pseudometric $\rho_\P$ induced by $\P$ is in fact a metric. We will return to the consequences of this choice in the next section.


\section{Continuity of probability measures}

Of course, the map $\P$ above is supposed to be analogous to a probability measure on a (concrete) $\sigma$-algebra. But ordinary measures are \emph{countably} additive, not just finitely so. It is however difficult to justify extending the finite additivity to sequences of disjoint events purely on conceptual or operational groups. In fact, according to Kolmogorov himself:
%
\blockquote[\cite{kolmogorov1956}]{%
    Since the new axiom \textins{the axiom of continuity} is essential for infinite fields of probability only, it is almost impossible to elucidate its empirical meaning. \textelp{} For, in describing any observable random process we can obtain only finite fields of probability. Infinite fields of probability occur only as idealized models of real random processes. \emph{We limit ourselves, arbitrarily, to only those models which satisfy Axiom VI} \textins{the axiom of continuity}. This limitation has been found expedient in researches of the most diverse sort.%
}
%
And furthermore:
%
\blockquote[\cite{kolmogorov1995}]{%
    \textins*{S}omewhat more complicated problems require, if the theory is to be simple and tractable, that probability be subject to the \emph{axiom of denumerable additivity}. However, the justification of that axiom remains purely empirical, in that we have not yet encountered any interesting problem for which we have not been able to construct a probability field conforming to the axiom in question.%
}
%
The axiom of continuity is, as we saw in \cref{thm:generalised-addition}, equivalent to countable additivity in our formalism. While countable additivity is usually preferred in the definition of (probability) measures today, \textcite{kolmogorov1956} instead assumed the axiom of continuity and then proved the generalised addition theorem, as we have done above.

Kolmogorov could apparently only find justification for assuming the axiom in the success of the theory, and not in its conceptual underpinnings. Luckily, in the present context we can avoid taking a stance: We have already heard Kolmogorov argue that the absense of non-empty events of probability zero is conceptually problematic; hence we have disallowed them by assuming our probability measures to be positive definite. And according to \cref{thm:positive-definite-implies-continuous}, in this setting we get the axiom of continuity for free.

We are still not assured that our probability spaces are closed under countable joins. However, \cref{thm:completion-gives-countable-additivity} tells us that taking the completion of a generalised abstract probability space solves the problem. And according to Kolmogorov, this can be done without issue:
%
\newlength{\oldparindent}
\setlength{\oldparindent}{\parindent}
\blockquote[\cite{kolmogorov1995}]{\setlength{\parskip}{0pt}\setlength{\parindent}{\oldparindent}
    The use of denumerable additivity, with its great concomitant freedom, is not generally possible except in \emph{complete} metric Boolean algebras. It is therefore natural, in probability theory, always to assume that the algebra of events is \emph{complete} by adjunction of ideal elements. As has been pointed out, this situation is always realizable.

    So, the use of denumerably additive probabilities is seen to be legitimate and to impose no additional restrictions on the nature of the problems which fall under the scope of the general theory.
}
%
Notice that he calls the adjoined elements \enquote{ideal}. Thus he seems to think of these new events as not (necessarily) corresponding to real, observable events as described above, or at least not events that we can gain any information about. Instead, he seems only to be concerned with how their adjunction can improve the flexibility of the theory without restricting which classes of problems the theory can address. Indeed, he had the following to say:
%
\blockquote[\cite{kolmogorov1956}]{\setlength{\parskip}{0pt}\setlength{\parindent}{\oldparindent}
    Even if the sets (events) $A$ of $\mathfrak{F}$ can be interpreted as actual and (perhaps only approximately) observable events, it does not, of course, follow from this that the sets of the extended field $B\mathfrak{F}$ [the $\sigma$-algebra generated by $\mathfrak{F}$] reasonably admit of such an interpretation.

    Thus there is the possibility that while a field of probability $(\mathfrak{F}, \mathsf{P})$ may be regarded as the image (idealized, however) of actual random events, the extended field of probability $(B\mathfrak{F}, \mathsf{P})$ will still remain merely a mathematical structure.
    
    Thus sets of $B\mathfrak{F}$ are generally merely ideal events to which nothing corresponds in the outside world. However, if reasoning which utilizes the probabilities of such ideal events leads us to a determination of the probability of an actual event of $\mathfrak{F}$, then, from an empirical point of view also, this determination will automatically fail to be contradictory.
}
%
And indeed, upon taking the completion of a probability space we always have an isometric copy of the original space inside the new one.

We summarise the result of the above discussion in the following definition, which is the one we will be concerned with going forward:

\begin{definition}[Abstract probability spaces]
    \label{def:abstract-probability-space}
    An \keyword{abstract probability space} is a generalised abstract probability space $(\calF, \P)$, where $\calF$ is an abstract $\sigma$-algebra.
\end{definition}


\chapter{Set-theoretical probability theory}

\section{Basic definitions and properties}

We begin by recalling the standard definitions in order to compare them to the abstract versions we have considered above:

\begin{definition}[Measurable spaces]
    A \keyword{(concrete) $\sigma$-algebra} in a set $\Omega$ is a collection $\calF$ of subsets of $\Omega$ such that
    %
    \begin{enumdef}
        \item $\Omega \in \calF$,
        \item $A \in \calF$ implies $A^c \in \calF$, and
        \item if $(A_n)_{n\in\naturals}$ is a sequence in $\calF$, then $\bigunion_{n\in\naturals} A_n \in \calF$.
    \end{enumdef}
    %
    The sets in $\calF$ are called \keyword{$\calF$-measurable}, and the pair $(\Omega, \calF)$ is called a \keyword{measurable space}.
\end{definition}
%
Note that a $\sigma$-algebra by this definition is indeed an \emph{abstract} $\sigma$-algebra in the sense of \cref{def:abstract-sigma-algebra}. Also contrast the definition of a $\sigma$-algebra with that of a \emph{set algebra}: in the latter case we also require it to contain $\Omega$ and be closed under complementation, but we only require it to be closed under \emph{finite} unions. In particular a set algebra is a Boolean algebra.


\begin{definition}[Measure spaces and probability spaces]
    Let $(\Omega, \calF)$ be a measurable space. A \keyword{measure} on $(\Omega, \calF)$ is a map $\mu \colon \calF \to [0,\infty]$ such that
    %
    \begin{enumdef}
        \item $\mu(\emptyset) = 0$, and
        \item $\mu$ is countably additive, i.e. for every sequence $(A_n)_{n\in\naturals}$ of pairwise disjoint sets in $\calF$ we have
        %
        \begin{equation*}
            \mu \Bigl( \bigunion_{n\in\naturals} A_n \Bigr)
                = \sum_{n=1}^\infty \mu(A_n).
        \end{equation*}
    \end{enumdef}
    %
    The triple $(\Omega, \calF, \mu)$ is called a \keyword{(concrete) measure space}. If $\mu(\Omega) = 1$ then $\mu$ is called a \keyword{probability measure} and the triple $(\Omega, \calF, \mu)$ a \keyword{(concrete) probability space}.
\end{definition}
%
Again, if $(\Omega, \calF, \mu)$ is a measure space thus defined, then $(\calF, \mu)$ is also an \emph{abstract} measure space of the type considered in \cref{def:abstract-measure-space}. However, even if $\mu$ is a probability measure, $(\calF, \mu)$ is not necessarily an \emph{abstract} probability space as defined in \cref{def:abstract-probability-space}.

The trouble is that $\mu(A) = 0$ does not necessarily imply that $A = \emptyset$, i.e. $\mu$ need not be positive definite. In an abstract measure space (perhaps of the generalised kind) $(\calF,\mu)$, the only information we have about the relationship between two elements $x,y \in \calF$ is whether they are comparable and, if so, which one is greater.\footnote{Recall that the ordering on a lattice completely determines the lattice structure.} Or in other terms for events $E$ and $F$, whether one of them implies the other or not.

By contrast, elements of a concrete $\sigma$-algebra are subsets of an underlying set. This has several important implications: First of all, this vastly increases the number of objects we have to contend with, namely \emph{every} subset of $\Omega$, not just those lying in $\calF$. This may have benefits of a technical nature; certainly it would have simplified many of the arguments in the previous section on Boolean algebras.

Secondly, it forces us to take the very elements of the \emph{sample space} $\Omega$ seriously. Usually these are referred to as \emph{outcomes} or, e.g. by Kolmogorov, \emph{elementary events}. But this may be conceptually problematic:
%
\blockquote[\cite{kolmogorov1995}]{
    \textins*{T}he notion of an elementary event is an artificial superstructure imposed on the concrete notion of an event. In reality, events are not composed of elementary events, but elementary events originate in the dismemberment of composite events.
}
%
In other words, arriving at the idea of an elementary event requires \emph{analysis} of the collection of events that is given in some random experiment.

A simple example may help elucidate this point: Consider the random experiment consisting of rolling a six-sided die. Say this die is loaded in such a way that it is impossible to roll a $6$, and that it is very likely to roll a $1$ or a $2$. We construct an abstract probability space ($\calF, \P)$ to model the outcome of the die roll. The event space might be
%
\begin{equation*}
    \calF
        = \{ 0, A_1, \ldots, A_5, E, F, 1 \},
\end{equation*}
%
where we interpret the event $A_i$ as \enquote{the result was $i$} for $i = 1, \ldots, 5$, $E$ as \enquote{the result was even} and $F$ as \enquote{the result was odd}. There is no event $A_6$ since it is impossible to roll a $6$. Certainly we would require that
%
\begin{enumerate}
    \item $A_1, \ldots, A_5$ be pairwise disjoint,
    \item $E = A_2 \join A_4$ and $F = A_1 \join A_3 \join A_5$ (from which it follows that $E$ and $F$ are disjoint), and
    \item $E \join F = 1$.
\end{enumerate}
%
Assigning probabilities to each event we might find that $\P(A_1) = \P(A_2) = 0.35$, and that $\P(A_3) = \P(A_4) = \P(A_5) = 0.1$. Additivity of $\P$ would then imply that $\P(E) = 0.45$ and $\P(F) = 0.55$.

It is tempting to \emph{identify} each of $E$ and $F$ with the (possible) events that imply them, i.e. identify $E$ with the set $\{A_2, A_4\}$ and $F$ with the set $\{A_1, A_3, A_4\}$ of events. Maybe we are even tempted to include a hypothetical event $A_6$ in the former set. But notice that this is an \emph{analysis} of the events $E$ and $F$. It would be quite possible to grasp the meaning of the event $E$ without immediately enumerating each of the \enquote{elementary} events it consists of. Indeed events in even set-theoretic probability theory (and objects in mathematics as a whole, for that matter) are often defined in terms of their properties, not of their constituents. Or to put it in other terms: by their \emph{intension} and not their \emph{extension}.

On the other hand, the set-theoretic approach and the possibility of non-empty null sets offer certain conceptual advantages as well. Perhaps we would like there to be an event $A_6$ even if it is impossible. Certainly our model of the die seems incomplete without it; the die is, after all, six-sided! Admittedly this picture seems rather artificial since the probability of rolling a $6$ using a physical die is surely positive, however small it happens to be. So imagine that this die appears in a video game or in a fantasy novel where this might be a more easily digestible proposition.

Furthermore, if we actually do want to use our theory of probability to \emph{model} physical phenomena, then it might be perfectly reasonable to include some event in the model on conceptual grounds, even if this event happens to be assigned probability zero in the model. Perhaps this fact is due to numerical approximation, missing information or a state of affairs that comes about after each event under consideration has been identified. Indeed, we may consider a (concrete or abstract) $\sigma$-algebra and interpret its elements as events, even if there is no particular probability measure defined on it. Thus the system of events seems in some sense prior to the assignment of probabilities to the events.


\section[Extending premeasures to sigma-algebras][Extending premeasures to $\sigma$-algebras]{Extending premeasures to $\sigma$-algebras}

Recall that we in \cref{thm:completion-gives-countable-additivity} proved that the completion of a generalised abstract measure space $(B,\mu)$ is an abstract measure space under the assumption that $\mu$ is positive definite. Since this is no longer the case we need another way of extending a finitely additive probability measure on a set algebra to a $\sigma$-algebra. Furthermore, in \cref{thm:positive-definite-implies-continuous} we showed that $(B,\mu)$ automatically satisfies the axiom of continuity if $\mu$ is positive definite. We will thus also have to overcome this obstacle.

% Kolmogorov, now introduces axiom of continuity. Premeasures and Caratheodory, no issue if we assume the axiom! Conceptual reasons for assuming this?
% Rename section, perhaps something about continuity.

We begin with the latter: Let $\calA$ be an algebra on a set $\Omega$, and let $\mu_0 \colon \calA \to [0,\infty]$ be a \keyword{premeasure} on $\calA$. Recall that this means that $\mu_0(\emptyset) = 0$, and that
%
\begin{equation*}
    \mu_0 \Bigl( \bigunion_{n\in\naturals} A_n \Bigr)
        = \sum_{n=1}^\infty \mu_0(A_n)
\end{equation*}
%
for any sequence $(A_n)_{n\in\naturals}$ of pairwise disjoint sets in $\calA$ such that $\bigunion_{n\in\naturals} A_n \in \calA$. Notice that if $\mu_0$ is finite, this says exactly that $(\calA,\mu_0)$ is a generalised abstract measure space. But if $\mu_0$ is finite, then countable additivity of the above sort follows from \cref{thm:generalised-addition} if only $(\calA,\mu_0)$ satisfies the axiom of continuity.

We will attempt to motivate the axiom of continuity for a generalised abstract probability space $(\calF, \P)$. In this setting the axiom says that, given a decreasing sequence of events $(E_n)_{n\in\naturals}$ in $\calF$ such that $\bigmeet_{n\in\naturals} E_n = 0$, we have $\lim_{n\to\infty} \P(E_n) = 0$.

If $(E_n)$ is eventually constant, $E_n$ must equal $0$ for large enough $n$, in which case the axiom is obvious. If instead all $E_n$ are possible, the assumption that $\lim_{n\to\infty} \P(E_n) = 0$ says that it is still impossible for all $E_n$ to obtain. Since the sequence is decreasing, this must mean that the $E_n$ become increasingly less likely to occur, and the probability that $E_n$ occurs must get vanishingly small as $n$ tends to infinity. In other words, $\P(E_n)$ must approach zero in the limit $n \to 0$.

Of course this is not a proof of the axiom of continuity, but it illustrates that it a quite natural assumption. Thus we will henceforth assume that $\mu_0$ is indeed a premeasure on a set algebra $\calA$.

Next, recall that if $\calJ$ is a collection of subsets of $\Omega$ containing $\emptyset$ and $\mu \colon \calJ \to [0,\infty]$ satisfies $\mu(\emptyset) = 0$, then
%
\begin{equation}
    \label{eq:outer-measure}
    \mu^*(A)
        = \inf \set[\Big]{
            \sum_{n=1}^\infty \mu(B_n)
        }{
            (B_n)_{n \in \naturals} \subseteq \calJ
            \text{ and }
            A \subseteq \bigunion_{n \in \naturals} B_n
        }
\end{equation}
%
defines an outer measure on $\Omega$. We denote the $\sigma$-algebra of $\mu^*$-measurable sets by $\calM(\mu^*)$. In the case $\calJ = \calA$ and $\mu = \mu_0$, the following theorem guarantees that we may extend $\mu_0$ to an almost unique countably additive measure:

\begin{theorem}
    Let $\mu_0$ be a premeasure on an algebra $\calA$ in a set $\Omega$, and let $\mu^*$ be given by \cref{eq:outer-measure} with $\mu = \mu_0$ and $\calJ = \calA$. Then $\calA \subseteq \calM(\mu^*)$, and $\mu^*|_\calA = \mu_0$.

    In particular, $\mu^*$ restricts to a measure $\mu$ on $\calF = \sigma(\calA)$ whose restriction to $\calA$ is $\mu_0$. If $\nu$ is another measure on $\calF$ that extends $\mu_0$, then $\nu(A) \leq \mu(A)$ for all $A \in \calF$, with equality when $\mu(A) < \infty$. If $\mu_0$ is $\sigma$-finite, then $\mu = \nu$.
\end{theorem}

\begin{proof}
    \textcite[Theorem~1.14]{folland2007}.
\end{proof}
%
Thus in the case where $\mu_0$ is a probability premeasure, there exists a \emph{unique} extension to a probability measure $\P$ on the $\sigma$-algebra $\calF$ generated by $\calA$.

To sum up: Given a finitely additive probability measure $\mu_0$ on a set algebra $\calA$ in a set $\Omega$, the pair $(\calA,\mu_0)$ is a generalised abstract measure space. However, since $\mu_0$ is not necessarily positive definite, it does not immediately extend to a measure on an abstract $\sigma$-algebra, the issue being that $(\calA,\mu_0)$ does not satisfy the axiom of continuity. Thus we must impose this axiom, and we have argued that this is reasonable. Under this assumption $\mu_0$ becomes a premeasure and thus extends uniquely to a measure $\P$ on $\calF = \sigma(\calA)$.

Thus we arrive at the usual conception of a probability space as a measure space $(\Omega,\calF,\P)$ with $\P$ a countably additive probability measure. However, we still don't really understand how to think about $\sigma$-algebras. Can we interpret $\sigma$-algebras in some way, or are they merely a convenience that doesn't mean anything in practice?


\section{Conditional probability and independence}
\label{sec:independence}

Let $(\Omega,\calF,\P)$ be a probability space. Given events $A,B \in \calF$ with $\P(B) > 0$, the \keyword{conditional probability of $A$ given $B$} is defined as
%
\begin{equation*}
    \condP{A}{B}
        = \frac{ \P(A \intersect B) }{ \P(B) }.
\end{equation*}
%
Notice that the map $A \mapsto \condP{A}{B}$ is a probability measure on $B$. We interpret it as the probability that $A$ obtains given the knowledge that $B$ has already occurred. In the frequentist interpretation of probability, if the appropriate random experiment is performed $N$ times, and $B$ occurs $n(B)$ times and $A \intersect B$ $n(A \intersect B)$ times, then we would expect that the probability that $A$ occurs \emph{given} $B$ to be approximated by the proportion of \enquote{successful} outcomes:
%
\begin{equation*}
    \condP{A}{B}
        \approx \frac{n(A \intersect B)}{n(A)}
        = \frac{n(A \intersect B)/N}{n(A)/N}
        \approx \frac{\P(A \intersect B)}{\P(B)}.
\end{equation*}

Another way to \enquote{derive} the formula for $\condP{A}{B}$ is to consider the measure $\P_B$ on the measurable space $(B,\calF_B)$, where $\calF_B = \set{A \intersect B}{A \in \calF}$, given by $\P_B(A) = k \P(A \intersect B)$, where $k > 0$ is a constant to be determined. Interpreting $\P_B(A)$ to be the conditional probability of $A$ given $B$, we would like $\P_B$ to be a probability measure. This requires that
%
\begin{equation*}
    1
        = \P_B(B)
        = k \P(B \intersect B)
        = k \P(B),
\end{equation*}
%
or equivalently that $k = 1/\P(B)$, thus recovering the formula above.

In the case that knowing that $B$ has occurred yields no knowledge about the likelihood of $A$, we would expect that $\condP{A}{B} = \P(A)$, or equivalently
%
\begin{equation*}
    \P(A \intersect B) = \P(A) \P(B).
\end{equation*}
%
That is, $A$ and $B$ are \emph{independent}:

\begin{definition}[Independence I]
    \label{def:independence-1}
    Let $I$ be a non-empty index set. A family $(A_i)_{i \in I}$ of events from $\calF$ is called \keyword{independent} relative to $\P$ if
    %
    \begin{equation*}
        \P \Bigl( \bigintersect_{\nu = 1}^n A_{i_\nu} \Bigr)
            = \bigprod_{\nu = 1}^n \P(A_{i_\nu}),
    \end{equation*}
    %
    for any finite subset $\{ i_1, \ldots, i_n \}$ of distinct elements of $I$.
\end{definition}
%
The intuition being that the knowledge that some $A_i$ has obtained does not affect our knowledge of the other $A_i$. This is clear in the case when $I$ contains two elements. To understand the extension to arbitrary collections of events, let us first look at the family $\{A, B, C\}$ of events from $\calF$.

If the events $A$ and $B$ are independent, then we interpret this as meaning that knowing that $A$ has occurred gives us no information about whether $B$ has occurred or not. That is, it does not allow us to approximate $\P(B)$ any better. Furthermore, if $\{A,B,C\}$ is independent, then we would similarly interpret this to mean that knowing that $A$ has occurred does not give us information about whether $B$ or $C$ have occurred. But certainly, if the collection $\{A,B,C\}$ is independent, then e.g. the subcollection $\{A,B\}$ should also be independent. So if $\{A,B,C\}$ is independent, then we would in particular have $\P(A \intersect B) = \P(A) \P(B)$.

Next we attempt to interpret, or derive, the identity $\P(A \intersect B \intersect C) = \P(A) \P(B) \P(C)$. We imagine doing an experiment to determine whether $A$ has occurred, and then doing another experiment to determine whether $B$ has occurred. Then we would know whether $A \intersect B$ has occurred, so if neither $A$ nor $B$ can provide information about $C$, then it seems reasonable to think that $A \intersect B$ cannot either.

Hence, if $\{A,B,C\}$ is to be independent, then $\{A \intersect B, C\}$ is also to be independent. Furthermore, we already argued that $\{A,B\}$ should be independent, so we find that
%
\begin{equation*}
    \P(A \intersect B \intersect C)
        = \P \bigl( (A \intersect B) \intersect C \bigr)
        = \P(A \intersect B) \P(C)
        = \P(A) \P(B) \P(C).
\end{equation*}
%
Hence this identity is a natural consequence of our interpretation of events and of independence.

This obviously extends to any finite collection of events. But what does it mean for an infinite collection of events to be independent? Above we justified requiring the identity $\P(A \intersect B \intersect C) = \P(A) \P(B) \P(C)$ in the definition of independence by appealing to an ability to perform experiments to judge whether the events $A,B,C$ had occurred. While it is clearly possible, at least in principle, to perform any finite number of experiments (as we argued in our discussion of the generalised abstract probability space of events), it is not so clear that we should be able to perform \emph{infinitely} many such experiments. Hence for an infinite collection $\calA$ of events to be independent, it seems to be sufficient that any finite subcollection of $\calA$ be independent, which is equivalent to \cref{def:independence-1}.

We may generalise this definition of independence in the following way:

\begin{definition}[Independence II]
    \label{def:independence-2}
    Let $(\calC_i)_{i \in I}$ be a family of sets $\calC_i \subseteq \calF$ of events. This family is called \keyword{independent} relative to $\P$ if
    %
    \begin{equation*}
        \P \Bigl( \bigintersect_{\nu = 1}^n A_{i_\nu} \Bigr)
            = \bigprod_{\nu = 1}^n \P(A_{i_\nu}),
    \end{equation*}
    %
    for any choice of events $A_{i_\nu} \in \calC_{i_\nu}$ and any finite subset $\{ i_1, \ldots, i_n \}$ of distinct elements of $I$.
\end{definition}

\begin{remark}
    \label{rem:finite-subfamilies-independent}
    Notice that, since the intersections and products above are finite, $(\calC_i)_{i \in I}$ is independent if and only if $(\calC_i)_{i \in J}$ is independent for every finite subset $J \subseteq I$.
\end{remark}
%
This definition reduces to the first one if all $\calC_i$ are singletons. But notice what this definition says: Choosing a single event $A_i$ from each $\calC_i$, these $A_i$ are independent. The definition does not imply any relationship between the events in each $\calC_i$. An example, adapted from Example~6.1 in \textcite{bauer1995}, will illustrate this point:


\begin{example}
    \label{ex:die-throw-1}
    Consider two throws of a fair die. The sample space is then $\Omega = \{1, \ldots, 6\}^2$, and we consider the probability space $(\Omega, \powerset{\Omega}, \P)$, where $\P$ assigns equal probability $1/36$ to each elementary event $\{(i,j)\}$. Let $A_1$ and $A_2$ be the events that on the first throw an even or odd number showed up, respectively, and let $A_3$ be the event that the sum of the two throws is odd. Now collect the three events in $\calC_1 = \{A_1, A_2\}$ and $\calC_2 = \{A_3\}$. Then $\calC_1$ and $\calC_2$ are independent even through $A_1$ and $A_2$ are not.
\end{example}
%
The question then becomes, why are we interested in this generalisation of independence? Independence of two collections $\calC_1$ and $\calC_2$ is supposed to mean that the information contained in each is in some way independent of the information contained in the other. By \enquote{information} we mean something like: the ability to better predict the outcome of a random experiment, or in other words improved knowledge of the probabilities of events.

Given that $\calC_1$ and $\calC_2$ are independent, is there any more we can say? Is it the case, for instance, that each event $\calC_1$ is independent of the (finite or countable) intersection of events in $\calC_2$? What about unions or complements? In the case of intersections the answer is negative:

\begin{example}
    \label{ex:die-throw-2}
    Let $(\Omega,\powerset{\Omega},\P)$ be the probability space defined in \cref{ex:die-throw-1}, and let $A_1$ and $A_3$ be the same events as before. But now let $A_2'$ be the event that the \emph{second} throw yielded an odd number, and put $\calC_1' = \{A_1, A_2'\}$. Again $\calC_1'$ and $\calC_2$ are easily seen to be independent, and indeed so are $A_1$ and $A_2'$. However, $A_1 \intersect A_2'$ and $A_3$ are manifestly not independent: For while $\P(A_1 \intersect A_2') = 1/4$ by independence, we have
    %
    \begin{equation*}
        \P \bigl( (A_1 \intersect A_2') \intersect A_3 \bigr)
            = 0,
    \end{equation*}
    %
    since if each throw turns up odd, the sum must be even.

    Incidentally, this also shows that the requirement in \cref{def:independence-1} that $\P$ be multiplicative for \emph{every} finite subset of $I$ is necessary; it is not enough that the events be \emph{pairwise} independent.
\end{example}

As for unions and complementation, we have slightly more success. To do this discussion justice we introduce another piece of terminology:

\begin{definition}[Dynkin systems]
    A collection $\calD$ of subsets of a set $X$ is called a \keyword{Dynkin system} in $X$ if
    %
    \begin{enumdef}
        \item $X \in \calD$,
        \item $B \setminus A \in \calD$ for $A,B \in \calD$ with $A \subseteq B$, and
        \item $\bigunion_{n\in\naturals} A_n \in \calD$ for every increasing sequence $(A_n)_{n\in\naturals}$ of sets in $\calD$.
    \end{enumdef}
\end{definition}
%
A Dynkin system is also variously called a Dynkin class, $\delta$-system, $d$-system, or $\lambda$-system. Clearly every $\sigma$-algebra is a Dynkin system. If $\calS$ is a collection of subsets of $X$, then there is a smallest Dynkin system in $X$ that contains $\calS$, namely the intersection of all such Dynkin systems. We denote this by $\delta(\calS)$ and say that it is \keyword{generated} by $\calS$.

The motivation for considering Dynkin systems is twofold. First of all they are significantly simpler than $\sigma$-algebras, and working with Dynkin systems instead of $\sigma$-algebras can often we done with no loss of generality, as the following fundamental result shows:

\begin{theorem}[Dynkin's Lemma]
    Let $\calS$ be a collection of subsets of a set $X$ that is closed under finite intersections. Then
    %
    \begin{equation*}
        \delta(\calS) = \sigma(\calS).
    \end{equation*}
\end{theorem}
%
Also known as \keyword{Dynkin's $\pi$-$\lambda$ theorem} since a non-empty collection of sets that is closed under finite intersections is also called a $\pi$-system.

\begin{proof}
    \textcite[Theorem~1.6.2]{cohn2001}. See also \textcite[Theorem~2.3]{bauer2001}, though note that Bauer uses a slightly different definition of Dynkin systems.
\end{proof}

Another source of motivation comes from the following result about finite measures which is important when proving uniqueness of properties of finite or $\sigma$-finite measures, but will also be of interest to us below:

\begin{lemma}
    \label{thm:finite-measures-agree-on-Dynkin}
    Let $\mu$ and $\nu$ be finite measures on a measurable space $(X,\calE)$ such that $\mu(X) = \nu(X)$. The family $\calD \subseteq \calE$ of sets on which $\mu$ and $\nu$ agree is a Dynkin system.
\end{lemma}

\begin{proof}
    By assumption $X \in \calD$. Let $A_1, A_2 \in \calD$ with $A_1 \subseteq A_2$. Then
    %
    \begin{equation*}
        \mu(A_2 \setminus A_1)
            = \mu(A_2) - \mu(A_1)
            = \nu(A_2) - \nu(A_1)
            = \nu(A_2 \setminus A_1),
    \end{equation*}
    %
    since $\mu$ and $\nu$ are finite. Finally assume that $(A_n)_{n\in\naturals}$ is an increasing sequence of elements in $\calD$. Then by continuity we have
    %
    \begin{equation*}
        \mu \Bigl( \bigunion_{n\in\naturals} A_n \Bigr)
            = \lim_{n\to\infty} \mu(A_n)
            = \lim_{n\to\infty} \nu(A_n)
            = \nu \Bigl( \bigunion_{n\in\naturals} A_n \Bigr).
    \end{equation*}
    %
    Thus $\calD$ is a Dynkin system as claimed.
\end{proof}

Our motivation for considering Dynkin systems, however, is the following result:

\begin{proposition}
    \label{thm:Dynkin-independence}
    Let $(\calC_i)_{i \in I}$ be an independent family of sets of events from $\calF$. Then the family $(\delta(\calC_i))_{i \in I}$ is also independent. In particular, if the $\calC_i$ are closed under intersection, the family $(\sigma(\calC_i))_{i \in I}$ is independent.
\end{proposition}

\begin{proof}
    Fix an index $i_0 \in I$, and choose sets $A_{i_\nu} \in \calC_{i_\nu}$ for distinct indices $i_1, \ldots, i_n \in I \setminus \{i_0\}$. Then define measures $\P_1$ and $\P_2$ on $\calF$ by
    %
    \begin{equation*}
        \P_1(A)
            = \P \Bigl( A \intersect \bigintersect_{\nu = 1}^n A_{i_\nu} \Bigr)
        \quad \text{and} \quad
        \P_2(A)
            = \P(A) \bigprod_{\nu = 1}^n \P(A_{i_\nu}),
    \end{equation*}
    %
    for $A \in \calF$. Notice that $\P_1$ and $\P_2$ agree on $\calC_{i_0}$ by independence, so since $\P_1(\Omega) = \P_2(\Omega)$, \cref{thm:finite-measures-agree-on-Dynkin} implies that they also agree on $\delta(\calC_{i_0})$. It follows that
    %
    \begin{equation*}
        \P \Bigl( A \intersect \bigintersect_{\nu = 1}^n A_{i_\nu} \Bigr)
            = \P(A) \bigprod_{\nu = 1}^n \P(A_{i_\nu})
    \end{equation*}
    %
    for all choices of sets $A_{i_\nu} \in \calC_{i_\nu}$. But this precisely expresses the independence of the family $(\calC_i)_{i \in I}$ with $\calC_{i_0}$ replaced by $\delta(\calC_{i_0})$.

    By \cref{rem:finite-subfamilies-independent} we may assume that $I$ is finite. Thus performing a finite number of such replacements, once for each index in $I$, proves the first claim. The second claim follows by Dynkin's lemma.
\end{proof}


\begin{proposition}[Combining $\sigma$-algebras]
    \label{prop:combining-sigma-algebras}
    Let $(\calC_i)_{i \in I}$ be an independent family of $\intersect$-stable sets $\calC_i \subseteq \calF$, and let $(I_j)_{j \in J}$ be a partition of $I$. If $\calG_j = \sigma( \bigunion_{i \in I_j} \calC_i)$, then $(\calG_j)_{j \in J}$ is independent.
\end{proposition}

\begin{proof}
    Let $\tilde{\calC}_j$ denote the collection of sets
    %
    \begin{equation*}
        A_{i_1} \intersect \cdots \intersect A_{i_n},
    \end{equation*}
    %
    where $i_1, \ldots, i_n$ are distinct elements from $I_j$, and $A_{i_\nu} \in \calC_{i_\nu}$. Then $\tilde{\calC}_j$ is $\intersect$-stable, and the family $(\tilde{\calC}_j)_{j \in I_j}$ is independent. We clearly have $\calG_j = \sigma(\tilde{\calC}_j)$, so \cref{thm:Dynkin-independence} implies that $(\calG_j)_{j \in J}$ is independent.
\end{proof}


\section[Sigma-algebras and information][$\sigma$-algebras and information]{$\sigma$-algebras and information}

We interpret \cref{thm:Dynkin-independence} as follows: Say that we are interested in an event $A \in \calC_1$. The independence of $\calC_1$ and $\calC_2$ tells us that no single event $B \in \calC_2$ can provide us with information about $A$. The result above then implies that neither can any event in $\delta(\calC_2)$. In particular, taking complements and increasing countable unions cannot give us information that was available in a single event in $\calC_2$ to begin with.

But if we are thinking of $\calC_2$ as information, then surely it makes sense to consider two different events $B_1, B_2 \in \calC_2$ simultaneously. After all, knowing whether $B_1$ and $B_2$ each have occurred we can conclude whether $B_1 \union B_2$ and $B_1 \intersect B_2$ have occurred as well. In other words, $\calC_2$ must be a set algebra if it is to model information. But then \cref{thm:Dynkin-independence} implies that $\calC_1$ and $\sigma(\calC_2)$ are independent: no information in $\sigma(\calC_2)$ can improve our knowledge of $A$ if no single event in the algebra $\calC_2$ can.

In some sense then, if $\calA$ is a set algebra (so in particular $\calA$ is closed under intersection) in $\Omega$, the generated $\sigma$-algebra $\sigma(\calA)$ carries no more information than $\calA$. This picture is not quite complete, of course: If $\calA$ is unable to give us \emph{any information whatsoever} about some event $E$, then $\sigma(\calA)$ cannot either. But why should that mean that there is no increase in information at all when passing from $\calA$ to $\sigma(\calA)$? Might there not be some other way of cashing out this idea of \enquote{information} than the ability to predict the probability of events?

To further probe the interpretation of $\sigma$-algebras as information, we introduce the following terminology:

\begin{definition}
    Let $\calC$ be a collection of subsets of a set $\Omega$. We say that points $\omega, \omega' \in \Omega$ are \keyword{$\calC$-equivalent} if, for every $A \in \calC$, they both lie in $A$ or $A^c$, i.e. if $\indicator{A}(\omega) = \indicator{A}(\omega')$. In this case we write $\omega \sim_\calC \omega'$.

    The relation $\sim_\calC$ induces a partition of $\Omega$ called the \keyword{$\calC$-partition}.
\end{definition}
%
Another way to put this is that $\omega$ and $\omega'$ lie in all the same sets in $\calC$. Compare this with the \emph{topological indistinguishability} relation from point-set topology: Two points $x,y$ in a topological space $(X,\calT)$ are called topologically indistinguishable if they have all the same (open) neighbourhoods, i.e. if every open set either contains both $x$ and $y$ or contains neither. This is then the same as $x$ and $y$ being $\calT$-equivalent, or in the notation above, $x \sim_\calT y$.

\begin{lemma}
    Given points $\omega, \omega' \in \Omega$ we have $\omega \sim_\calC \omega'$ if and only if $\omega \sim_{\sigma(\calC)} \omega'$. In particular, the $\calC$- and $\sigma(\calC)$-partitions of $\Omega$ coincide.
\end{lemma}

\begin{proof}
    The latter clearly implies the former, so assume that $\omega \sim_\calC \omega'$. The collection of subsets $A \subseteq \Omega$ such that $\indicator{A}(\omega) = \indicator{A}(\omega')$ is clearly a $\sigma$-algebra, and it contains $\calC$ by assumption. But then it must also contain $\sigma(\calC)$, so $\omega \sim_{\sigma(\calC)} \omega'$.
\end{proof}
%
Now consider drawing a random element $\omega$ from $\Omega$. If an observer has the information $\calC$, i.e. they know whether or not $\omega \in A$ for all $A \in \calC$, then all they know is which $\calC$-equivalence class $\omega$ lies in. But by the lemma, this class is the same as the $\sigma(\calC)$-equivalence class of $\omega$, so again passing from $\calC$ to $\sigma(\calC)$ yields no new information.

% In general, the smaller a set $A$ is, the more information one gains by knowing that $\omega \in A$. Furthermore, the more sets one has at one's disposal, i.e. the larger $\calC$ is, the more information one has about $\omega$.

Thus it really does not seem like $\sigma$-algebras can do any more work than the algebras that generate them. And since we are comfortable with algebras carrying some kind of information, maybe $\sigma$-algebras do too.

To show that we cannot indiscriminately think of $\sigma$-algebras as information, we give an example of a case in which we seem to have both no information and a lot of information. This is Example~4.10 in \textcite{billingsley1995}.

\begin{example}
    Consider the probability space $([0,1], \calF, \lambda)$, where $\calF$ is the Borel algebra $\borel{[0,1]}$ and $\lambda$ is the Lebesgue measure restricted to $[0,1]$. Furthermore, let $\calG$ be the sub-$\sigma$-algebra of $\calF$ consisting of countable and cocountable sets. Then the measure of each element in $\calG$ is either $0$ or $1$, so $\calF$ and $\calG$ are independent. Given some event $E \in \calF$,
    %
    \begin{enumerate}[label=\normalfont(\alph*)]
        \item $\calF$ contains \emph{no} information about $E$, in the sense that $E$ is independent of $\calF$.
    \end{enumerate}
    %
    On the other hand, the $\calG$-equivalence classes are singletons, so
    %
    \begin{enumerate}[resume, label=\normalfont(\alph*)]
        \item $\calF$ contains \emph{all} the information about $E$, for given $\calF$ an observer knows precisely which $\omega$ was drawn, hence whether $E$ occurred or not.
    \end{enumerate}
    %
    These are in apparent contradiction, so it must not be the case that we can always interpret $\sigma$-algebras as information.

    Of course this example is rather artificial (indeed $\calG$ is not even countably generated, and $\lambda$ restricted to $\calG$ is also almost trivial), and it should not be seen as prohibiting the interpretation of $\sigma$-algebras as information entirely.
\end{example}


\section{Kolmogorov's 0-1 law}

Let $\calF$ be a $\sigma$-algebra, and let $(\calF_n)_{n \in \naturals}$ be a sequence of $\sigma$-algebras contained in $\calF$. For $n \in \naturals$ define $\sigma$-algebras
%
\begin{equation*}
    \calT_n
        = \bigjoin_{i \leq n} \calF_i
        = \sigma \Bigl( \bigunion_{i \leq n} \calF_i \Bigr)
    \quad \text{and} \quad
    \calT^n
        = \bigjoin_{n < i} \calF_i
        = \sigma \Bigl( \bigunion_{n < i} \calF_i \Bigr),
\end{equation*}
%
and further define
%
\begin{equation*}
    \calT_\infty
        = \bigjoin_{n \in \naturals} \calT_n
        = \sigma \Bigl( \bigunion_{n \in \naturals} \calT_n \Bigr)
    \quad \text{and} \quad
    \calT^\infty
        = \bigmeet_{n \in \naturals} \calT^n
        = \bigintersect_{n \in \naturals} \calT^n.
\end{equation*}
%
We call $\calT_n$ the \keyword{past of $\calF_n$} and $\calT^n$ the \keyword{future of $\calF_n$}, and we furthermore call $\calF^\infty$ the \keyword{total history} and $\calT^\infty$ the \keyword{ultimative future} or the \keyword{tail-$\sigma$-algebra} of the sequence $(\calF_n)$. Notice that $\calT^\infty \subseteq \calT^1 \subseteq \calT_\infty$, where the last inclusion follows since $\calF_i \subseteq \calT_{i+1} \subseteq \calT_\infty$ for all $i \in \naturals$.


\begin{lemma}
    \label{lem:tail-sigma-algebra-independence}
    In the notation above, if the sequence $(\calF_n)$ is independent, then for each $n \in \naturals \union \{\infty\}$ the $\sigma$-algebras $\calT_n$ and $\calT^n$ are independent.
\end{lemma}

\begin{proof}
    This follows from \cref{prop:combining-sigma-algebras} for $n \in \naturals$, and hence $\calT_n$ and $\calT^\infty$ are also independent. Now notice that $\tilde{\calT} = \bigunion_{n \in \naturals} \calT_n$ is $\intersect$-stable: For if $A,B \in \tilde{\calT}$, then since the sequence $(\calT_n)_{n \in \naturals}$ is increasing, $A$ and $B$ lie in a common $\calT_n$, and this is obviously $\intersect$-stable. But since $\tilde{\calT}$ and $\calT^\infty$ are independent, it follows from \cref{thm:Dynkin-independence} that $\calT_\infty = \sigma(\tilde{\calT})$ and $\calT^\infty$ are also independent.
\end{proof}


\begin{theorem}[Kolmogorov's 0-1 law]
    Let $(\Omega,\calF,P)$ be a probability space, and let $(\calF_n)_{n \in \naturals}$ be an independent sequence of $\sigma$-algebras $\calF_n \subseteq \calF$. Then $P(A) \in \{0,1\}$ for all $A \in \calT^\infty$.
\end{theorem}

\begin{proof}
    Recall that $\calT^\infty \subseteq \calT_\infty$. Then $A \in \calT_\infty$, so \cref{lem:tail-sigma-algebra-independence} implies that $A$ is independent of itself. The claim follows.
\end{proof}



\section{Random variables}

\newcommand{\meas}[1]{\mathcal{M}(#1)}
\newcommand{\simplemeas}[1]{\mathcal{S\!M}(#1)}

There are various ways of motivating the definition of measurability of functions in general measure theory: In order to make possible integration, by analogy with continuous maps between topological spaces, or heuristically by appealing to an intuitive notion of measurability.

For random variables we do not have this luxury. Why should random variables be integrable? Why should random variables have anything to do with continuity? Hence we focus on the third point and try to explore what measurability means for random variables.

We begin by fixing terminology and notation: If $(X,\calE)$ and $(Y,\calF)$ are measure spaces, a map $f \colon X \to Y$ is said to be \keyword{$(\calE,\calF)$-measurable} if $f\preim(B) \in \calE$ for all $B \in \calF$. Denote by $\meas{\calE}$ the space of functions $X \to \reals$ that are $(\calE,\borel{X})$-measurable.

A \keyword{random variable} on a probability space $(\Omega,\calF,\P)$ is a function in $\meas{\calF}$. If a random variable $\rvar{X} \colon \Omega \to \reals$ is measurable, then in particular
%
\begin{equation*}
    \{ \rvar{X} = x \}
        \defn \set{\omega \in \Omega}{\rvar{X}(\omega) = x}
        \in \calF
\end{equation*}
%
for all $x \in \reals$. Thus the information in $\calF$ at least needs to determine on which sets $\rvar{X}$ takes on which values for $\rvar{X}$ to be measurable. The converse, however, is not the case:
%
\begin{example}
    Let $\calE$ be the countable, cocountable $\sigma$-algebra on $\reals$, and let $\rvar{X} \colon (\reals, \calE) \to (\reals, \borel{X})$ be the identity function. Then $\{\rvar{X} = x\} = \{x\} \in \calE$, but $\rvar{X}\preim([0,\infty)) = [0,\infty)$, which is neither countable nor cocountable. Hence $\rvar{X}$ has measurable fibres despite not being measurable.
\end{example}
%
Is measurability then too strong a condition to impose on a random variable? Let us imagine that some outcome $\omega \in \Omega$ has obtained, and that there is enough information in $\calF$ to determine $\rvar{X}(\omega)$. Then certainly we should be able to answer questions on the form \enquote{is $\rvar{X}(\omega) < a$?} for $a \in \rationals$. That is, we require that $\rvar{X}\preim((-\infty,a)) \in \calF$. But since sets on the form $(-\infty,a)$ for $a \in \rationals$ generate $\borel{\reals}$, this implies that $\rvar{X}$ is measurable. Thus measurability seems to be a consequence of our claim that $\calF$ should determine the values that $\rvar{X}$ takes.

Let us again say that some outcome $\omega \in \Omega$ has obtained, and that we are not able to simply observe the value $\rvar{X}(\omega)$. How do we then determine $\rvar{X}(\omega)$ using only the information in $\calF$? We have interpreted \enquote{information} in this sense as being able to answer questions on the form \enquote{is $\omega \in A$?} for $A \in \calF$. We will try to construct a procedure by which we can determine, or at least approximate, $\rvar{X}(\omega)$ only by asking questions on this form.

Of course we cannot ask whether $\omega \in \rvar{X}\preim(\rvar{X}(\omega))$, since we don't know $\rvar{X}(\omega)$. Furthermore, it seems unreasonable that we should be able to ask whether $\omega \in A$ for \emph{all} $A \in \calF$. Rather, it seems like we need a general way of calculating the value some random variable $\Omega \to \reals$ takes, given that $\omega$ has occurred.

Suppose for definiteness that we have drawn an element $\omega \in \Omega$ such that $x = \rvar{X}(\omega) > 0$, and that we wish to determine $x$ within some error $\epsilon > 0$. First, check if $x$ lies in the interval $(-\infty,n)$ for $n \in \naturals$, starting with $n = 1$. After a finite number of checks we find that $x \in I_0 \defn [n_0, n_0+1)$ for some $n_0 \in \naturals$. Next, we construct a sequence $(I_n)_{n\in\naturals}$ as follows: For $n \in \naturals_0$, let $a_n \in \rationals$ be the midpoint of $I_n$, and check if $x \in (-\infty, a_n)$. If this is the case, let $I_{n+1} = I_n \intersect (-\infty, a_n)$, and otherwise let $I_{n+1} = I_n \intersect [a_n, \infty)$. Since the interval length is halved in each step, then for some $N \in \naturals$ the length of $I_N$ is less than $\epsilon$. Hence we obtain in finitely many steps an approximation $a_N \in \rationals$ of $x$ such that $\abs{x-a_N} < \epsilon$.

Notice that we only used intervals on the form $(-\infty, a)$ with $a \in \rationals$ during this process. Thus only knowledge of the events $\{\rvar{X} \in (-\infty, a)\}$ is necessary to determine $\rvar{X}(\omega)$, at least approximately. And since the sets $(-\infty, a)$ generate the Borel algebra $\borel{\reals}$, this requirement is the same as requiring $\rvar{X}$ to be $(\calF,\borel{\reals})$-measurable.

In total, we arrive at the interpretation that a random variable $\rvar{X} \colon \Omega \to \reals$ is $\calF$-measurable if and only if $\calF$ contains enough information to determine the values that $\rvar{X}$ takes. The minimal information required is then $\sigma(\rvar{X})$, the $\sigma$-algebra on $\Omega$ generated by $\rvar{X}$.


\subsection{The factorisation lemma}

We prove a fundamental lemma that helps shed further light on the interpretation of measurability of random variables.


\begin{proposition}[The factorisation lemma]
    Let $X$ be a set, let $(Y,\calF)$ be a measurable space, and let $\phi \colon X \to Y$ be a map. Equip $X$ with the $\sigma$-algebra $\calE = \sigma(\phi)$ generated by $\phi$. Then
    %
    \begin{equation}
        \label{eq:factorisation-lemma}
        \meas{\calE}
            = \set{g \circ \phi}{g \in \meas{\calF}}.
    \end{equation}
    %
    That is, for every $f \in \meas{\calE}$ there exists a $g \in \meas{\calF}$ such that the diagram
    %
    \begin{equation*}
        \begin{tikzcd}[column sep=1em]
            X
                \ar[rr, "\phi"]
                \ar[dr, "f", swap]
            && Y
                \ar[dl, "g"] \\
            & \reals
        \end{tikzcd}
    \end{equation*}
    %
    commutes.
\end{proposition}
%
In other words, a function $f \colon X \to \reals$ is $\sigma(\phi)$-measurable if and only if it factors through $\phi$.

\begin{proof}
    Denote the set on the right-hand side of \cref{eq:factorisation-lemma} by $W$. It is clear that $W$ is a subspace of $\meas{\calE}$. If $A \in \calE$, then $A = \phi\preim(B)$ for some $B \in \calF$. Notice then that
    %
    \begin{equation*}
        \indicator{A}
            = \indicator{\phi\preim(B)}
            = \indicator{B} \circ \phi
            \in \calF,
    \end{equation*}
    %
    so $\indicator{A} \in W$. Hence $W$ contains all simple $\calE$-measurable functions.
    
    Now let $f \in \meas{\calE}^+$ be a non-negative function. Then there exists an increasing sequence $(f_n)_{n\in\naturals}$ of simple functions, i.e. functions in $W$, such that $f$ is the pointwise limit of $(f_n)$. For each $n\in\naturals$ there is a function $g_n \in \meas{\calF}$ such that $f_n = g_n \circ \phi$. Hence we have for each $x \in X$,
    %
    \begin{equation*}
        f(x)
            = \sup_{n\in\naturals} f_n(x)
            = \sup_{n\in\naturals} g_n(\phi(x))
            = g(\phi(x)),
    \end{equation*}
    %
    where $g \colon Y \to \reals$ is given by $g = \sup_{n\in\naturals}g_n$. This is $\calF$-measurable as a function into the extended real line $\altoverline{\reals}$. Next let $B = \set{y \in Y}{g(y) = \infty}$. Then $\tilde{g} = g \indicator{B^c} \in \meas{\calF}$, and since $\phi(X) \subseteq B^c$ we have $f = \tilde{g} \circ \phi$ as desired.

    Finally let $f \in \meas{\calE}$ be an arbitrary measurable function, and write $f = f^+ - f^-$ where $f^+ = f \join 0$ and $f^- = -(f \meet 0)$. Applying the above to $f^+$ and $f^-$ yields functions $g^+, g^- \in \meas{\calF}$ such that $f^\pm = g^\pm \circ \phi$. Letting $g = g^+ - g^-$ we obtain $f = g \circ \phi$, and the theorem is proved.
\end{proof}
%
Now let $\rvar{X}$ and $\rvar{Y}$ be random variables on $(\Omega,\calF,\P)$. Then the factorisation lemma says that, if $\rvar{Y}$ is in fact $\sigma(\rvar{X})$-measurable, then there exists a measurable function $g \colon \reals \to \reals$ such that $\rvar{Y} = g(\rvar{X})$, i.e. such that the diagram
%
\begin{equation*}
    \begin{tikzcd}[column sep=1em]
        \Omega
            \ar[rr, "\rvar{X}"]
            \ar[dr, "\rvar{Y}", swap]
        && \reals
            \ar[dl, "g"] \\
        & \reals
    \end{tikzcd}
\end{equation*}
%
commutes. This is also clearly sufficient for $\rvar{Y}$ to be $\sigma(\rvar{X})$-measurable.

Let us try to understand this in terms of the information interpretation of measurability: Knowing $\sigma(\rvar{X})$ is the same as knowing the value $\rvar{X}(\omega)$ that $\rvar{X}$ takes at each $\omega \in \Omega$, or at least being able to approximate it. And an ability to, for all $\omega \in \Omega$, determine $\rvar{Y}(\omega)$ given the value $\rvar{X}(\omega)$ just means that there is a function $g \colon \reals \to \reals$ such that $\rvar{Y} = g(\rvar{X})$. Hence there is such a function $g$ just in case $\rvar{Y}$ is determined by -- that is, measurable with respect to -- $\sigma(\rvar{X})$.





\section[Sub-sigma-algebras and conditional expectation][Sub-$\sigma$-algebras and conditional expectation]{Sub-$\sigma$-algebras and conditional expectation}

If $(\Omega, \calF, \P)$ is a probability space, then we have seen that we, at least to some degree, can think of $\calF$ as the amount of information we have available. More precisely, given any event $A \in \calF$ we are able to decide whether $A$ has occurred or not. Furthermore, a map $\rvar{X} \colon \Omega \to \reals$ being $\calF$-measurable means that, given $\omega \in \Omega$, we are able to approximate $\rvar{X}(\omega)$ to arbitrary precision in finitely many steps.

But what happens if we do not have access to all the information in $\calF$, but only to the information in some sub-$\sigma$-algebra $\calB \subseteq \calF$? If $\rvar{X}$ is not $\calB$-measurable, then there is some $a \in \rationals$ (since the intervals with rational endpoints generate $\borel{\reals}$) such that we cannot even tell whether $\rvar{X} < a$ or not. We wish to construct a random variable that in some sense is the best approximation of $\rvar{X}$, using only the information in $\calB$.

First let $B \in \calB$ be an event with $\P(B) > 0$. We define the \keyword{conditional expectation of $\rvar{X}$ given $B$} by
%
\begin{equation*}
    \condexp{\rvar{X}}{B}
        = \frac{\mean{\rvar{X} \indicator{B}}}{\P(B)}
        = \frac{1}{\P(B)} \int_B \rvar{X} \dif\P.
\end{equation*}
%
In other words, $\condexp{\rvar{X}}{B}$ is the mean of $\rvar{X}$ with respect to the probability measure $A \mapsto \condP{A}{B}$. If instead $\P(B) = 0$, then we let $\condexp{\rvar{X}}{B} = 0$ for simplicity (an arbitrary real number would work). If the mean $\mean{X}$ is the best approximation of $\rvar{X}$ given no further information, we interpret $\condexp{\rvar{X}}{B}$ as the best approximation of $\rvar{X}$ given that $B$ has occurred. Since we \emph{know} that $B$ has occurred there is nothing random about $\condexp{\rvar{X}}{B}$. The above of course requires that the mean of $\rvar{X}$ and $\rvar{X \indicator{B}}$ exist; to ensure this we will assume that $\rvar{X} \in \calL^1(\P)$.

Next let $\calB$ be the sub-$\sigma$-algebra of $\calF$ generated by $B$, i.e. $\calB = \{ \emptyset, B, B^c, \Omega \}$. We would then expect the conditional expectation of $\rvar{X}$ given $\calB$ to be the random variable $\condexp{\rvar{X}}{\calB}$ given by
%
\begin{equation}
    \label{eq:conditional-expectation-1}
    \condexp{\rvar{X}}{\calB}(\omega)
        =
        \begin{cases}
            \condexp{\rvar{X}}{B},   & \omega \in B, \\
            \condexp{\rvar{X}}{B^c}, & \omega \in B^c.
        \end{cases}
\end{equation}
%
That is, if we know that $\omega \in B$, then the best approximation of $\rvar{X}$ must be the conditional probability of $\rvar{X}$ given $B$, and similarly if $\omega \in B^c$. If more generally $\calB$ is finitely generated by events $B_1, \ldots, B_n$, then each $\omega \in \Omega$ lies in precisely one of $B_i$ and $B_i^c$ for $i = 1, \ldots, n$. That is, $\Omega$ can be partitioned into sets $B_1^* \intersect \cdots \intersect B_n^*$, where each $B_i^*$ is either $B_i$ or $B_i^c$. We would then have
%
\begin{equation*}
    \condexp{\rvar{X}}{\calB}(\omega)
        = \condexp{\rvar{X}}{ B_1^* \intersect \cdots \intersect B_n^* },
        \quad \text{for} \quad
        \omega \in B_1^* \intersect \cdots \intersect B_n^*.
\end{equation*}
%
Clearly this reduces to \cref{eq:conditional-expectation-1} when $n = 1$. Even more generally, let $\calF$ be a partition $\sigma$-algebra, say $\calF = \sigma( \set{B_i}{i \in I} )$ where $(B_i)_{i \in I}$ is a partition of $\Omega$. In this case we would expect that
%
\begin{equation*}
    \condexp{\rvar{X}}{\calB}
        = \sum_{i \in I} \condexp{\rvar{X}}{B_i} \indicator{B_i}.
\end{equation*}
%
Notice that the sum is actually finite at any given $\omega \in \Omega$ (indeed, $\omega$ lies in precisely one set $B_i$, so the $i$th term is the only one that is nonzero).

If $\calB$ is not a partition $\sigma$-algebra, it seems unlikely that we should be able to write down $\condexp{\rvar{X}}{\calB}$ explicitly in this case. Instead we will try to find properties that $\condexp{\rvar{X}}{\calB}$ must have for it to properly be called an approximation of $\rvar{X}$.

If nothing else, its mean should certainly agree with that of $\rvar{X}$. Furthermore, for it to be a proper approximation of $\rvar{X}$ it should probably also resemble $\rvar{X}$ locally. But we should be careful here: If we zoom in too far and let $\condexp{\rvar{X}}{\calB}$ \emph{equal} $\rvar{X}$ at each point, then we have gotten nowhere. Hence we should \enquote{zoom in} as far as the information in $\calB$ allows us to, but no further.

Furthermore, it is an easy theorem (see e.g. \cite[SÃ¦tning~10.2.1]{thorbjoernsen2014}), whose proof we will not reproduce here, that integrable functions whose integrals on any measurable set agree are equal almost everywhere. In probabilistic terms, $\calF$-measurable random variables $\rvar{X}, \rvar{Y} \in \calL^1(\P)$ are equal almost surely in the case that $\condexp{\rvar{X}}{B} = \condexp{\rvar{Y}}{B}$ for all $B \in \calF$. But in our case we do not have access to every event in $\calF$, only those that lie in $\calB$. This motivates the following definition, which is easily seen to be a generalisation of the ones we have considered so far:

\begin{definition}[Conditional expectations]
    \label{def:conditional-expectation}
    Let $(\Omega, \calF, \P)$ be a probability space, let $\calB$ be a sub-$\sigma$-algebra of $\calF$, and let $\rvar{X} \in \calL^1(\P)$ be a random variable.

    A \keyword{conditional expectation of $\rvar{X}$ given $\calB$} is a random variable $\rvar{U}$ on $(\Omega, \calF, \P)$ such that
    %
    \begin{enumdef}
        \item $\rvar{U} \in \calL^1(\P)$,
        \item $\rvar{U}$ is $\calB$-measurable, and
        \item \label{enum:conditional-expectation-local-approx} $\condexp{\rvar{U}}{B} = \condexp{\rvar{X}}{B}$ for all $B \in \calB$.
    \end{enumdef}
\end{definition}
%
Notice that condition \subcref{enum:conditional-expectation-local-approx} is equivalent to the requirement that
%
\begin{equation*}
    \int_B \rvar{U} \dif\P
        = \int_B \rvar{X} \dif\P
\end{equation*}
%
for all $B \in \calB$. The Radon--Nikodym theorem ensures the existence of a conditional expectation of any $\rvar{X} \in \calL^1(\P)$, and the arguments above show that it is uniquely determined $\P$-almost surely. We denote any conditional expectation of $\rvar{X}$ given $\calB$ by $\condexp{\rvar{X}}{\calB}$.

We cite without proof a few results that can help us judge whether \cref{def:conditional-expectation} gives us the approximation of $\rvar{X}$ that we wanted. The results can be found in any text on probability theory. Only the final property causes any difficulty: its proof uses the dominated convergence theorem for conditional expectations.

\begin{proposition}
    Let $\rvar{X} \in \calL^1(\P)$ be a random variable on a probability space $(\Omega, \calF, \P)$, and let $\calB$ be a sub-$\sigma$-algebra of $\calF$.
    %
    \begin{enumprop}
        \item \label{enum:conditional-expectation-no-information} If $\calB = \{\emptyset, \Omega\}$ then $\condexp{\rvar{X}}{\calB} = \mean{\rvar{X}}$.

        \item \label{enum:conditional-expectation-all-information} If $\rvar{X}$ is $\calB$-measurable, then $\condexp{\rvar{X}}{\calB} = \rvar{X}$ $\P$-a.s.

        \item \label{enum:conditional-expectation-constant} If $\rvar{U}$ is a $\calB$-measurable random variable such that $\rvar{U}\rvar{X} \in \calL^1(\P)$, then
        %
        \begin{equation*}
            \condexp{\rvar{U}\rvar{X}}{\calB}
                = \rvar{U} \condexp{\rvar{X}}{\calB}
                \quad \text{$\P$-a.s.}
        \end{equation*}
    \end{enumprop}
\end{proposition}
%
The trivial $\sigma$-algebra $\calB = \{\emptyset, \Omega\}$ is supposed to model the situation where we have no information: We only know that $\emptyset$ has not occurred and that $\Omega$ has. Hence $\condexp{\rvar{X}}{\calB}$ cannot depend on which events have occurred, so it must be constant. And the best constant approximation of $\rvar{X}$ is $\mean{\rvar{X}}$, hence \subcref{enum:conditional-expectation-no-information}.

In contrast, if $\rvar{X}$ is actually $\calB$-measurable as in \subcref{enum:conditional-expectation-all-information}, then $\calB$ contains all the information necessary to determine $\rvar{X}$, and the best approximation of $\rvar{X}$ that only depends on $\calB$ is just $\rvar{X}$ itself.

Finally consider the situation in \subcref{enum:conditional-expectation-constant}. One sometimes hears that $\calB$-measurable variables are \enquote{constant} when calculating conditional expectations with respect to $\calB$. This is true, as the result above shows, in the sense that we can \enquote{pull out} such variables from the conditional expectation. Another way to understand this is in relation to \subcref{enum:conditional-expectation-all-information}: Since $\rvar{U}$ is completely determined by the information in $\calB$, the best approximation is just $\rvar{U}$ itself. This result then says that, in this special case, the best approximation of the product $\rvar{U}\rvar{X}$ is the product of the best approximations of $\rvar{U}$ and $\rvar{X}$ respectively.


\subsection{The tower principle}

Say that we are given a probability space $(\Omega,\calF,\P)$ and \emph{two} sub-$\sigma$-algebras $\calB$ and $\calB_1$ of $\calF$. If neither of $\calB$ and $\calB_1$ is contained in the other, the information contained in them is in some sense incompatible. But if they are nested, we have the following result:

\begin{proposition}[The tower principle]
    \label{thm:tower-principle}
    Let $(\Omega,\calF,\P)$ be a probability space, let $\rvar{X} \in \calL^1(\P)$, and let $\calB$ and $\calB_1$ be sub-$\sigma$-algebras of $\calF$. If $\calB_1 \subseteq \calB$, then
    %
    \begin{equation*}
        \condexp[\big]{
            \condexp{\rvar{X}}{\calB}
        }{\calB_1}
        =
        \condexp{\rvar{X}}{\calB_1}
        =
        \condexp[\big]{
            \condexp{\rvar{X}}{\calB_1}
        }{\calB}
        \quad \text{$\P$-a.s.}
    \end{equation*}
\end{proposition}

\begin{proof}
    The first equality follows easily from the definition, and the second is a consequence of \cref{enum:conditional-expectation-all-information}.
\end{proof}
%
This result tells us that approximating $\rvar{X}$ using the information in $\calB$ and then afterwards approximating further using $\calB_1$ is the same as just approximating $\rvar{X}$ using $\calB_1$ directly.

Another way to understand this result is in terms of projections. Recall that we for $p \in (0,\infty)$ denote by $L^p(\P)$ the space of equivalence classes of random variables in $\calL^p(\P)$, where $\rvar{X}$ and $\rvar{Y}$ are equivalent if $\rvar{X} = \rvar{Y}$ $\P$-a.s. In the case that $\rvar{X}, \rvar{Y} \in \calL^1(\P)$ it is easy to show that if $\rvar{X} = \rvar{Y}$ $\P$-a.s., then also $\condexp{\rvar{X}}{\calB} = \condexp{\rvar{Y}}{\calB}$ $\P$-a.s. for any sub-$\sigma$-algebra $\calB$ of $\calF$. Hence $\calB$ induces a well-defined linear map
%
\begin{align*}
    P_\calB \colon L^1(\P) &\to L^1(\P), \\
    \rvar{X} &\mapsto \condexp{\rvar{X}}{\calB}.
\end{align*}
%
Here we have suppressed the distinction between elements of $\calL^1(\P)$ and of $L^1(\P)$. The content of \cref{thm:tower-principle} can then be phrased simply as
%
\begin{equation*}
    P_{\calB_1} \circ P_\calB
        = P_{\calB_1}
        = P_\calB \circ P_{\calB_1},
\end{equation*}
%
making the comparison with projection operators very tempting.

In fact, we can make this analogy more explicit. Let $\calL^p(\calB,\P)$ denote the subspace of $\calL^p(\P)$ consisting of $\calB$-measurable functions. Let $\P_\calB$ be the restriction of $\P$ to $\calB$. It is then not difficult to show that integrals of $\calB$-measurable functions with respect to $\P$ and $\P_\calB$ coincide, and hence that $\calL^p(\calB,\P) = \calL^p(\P_\calB)$. Taking equivalence classes we find that also\footnote{If $\rvar{X},\rvar{Y} \in \calL^p(\calB,\P)$ are two random variables, then the set on which they differ is $\calB$-measurable. Hence two $\calB$-measurable variables that belong to the same equivalence class differ on a $\calB$-measurable set.} $L^p(\calB,\P) = L^p(\P_\calB)$.

\begin{proposition}
    The map $P_\calB$ restricted to $L^2(\P)$ is the orthogonal projection onto $L^2(\calB,\P)$.
\end{proposition}

\begin{proof}
    First notice that $L^2(\calB,\P) = L^2(\P_\calB)$ is complete, so it is a closed subspace of $L^2(\P)$. Now let $\rvar{X} \in \calL^2(\P)$, and let $\mathcal{X}$ be the orthogonal projection of $[\rvar{X}]$ onto $L^2(\calB,\P)$. If $\rvar{U} \in \mathcal{X}$ then $[\rvar{X}] - [\rvar{U}] \in L^2(\calB,\P)^\perp$, so in particular $[\rvar{X}] - [\rvar{U}] \perp \indicator{B}$ for all $B \in \calB$. But this implies that
    %
    \begin{equation*}
        \int_B \rvar{X} \dif\P
            = \int_B \rvar{U} \dif\P,
    \end{equation*}
    %
    so $\rvar{U}$ is indeed a conditional expectation of $\rvar{X}$, i.e. $P_\calB([\rvar{X}]) = [\rvar{U}]$.
\end{proof}
%
In this setting \cref{thm:tower-principle} becomes obvious. Furthermore, in makes our desire that $\condexp{\rvar{X}}{\calB}$ be the \emph{best} approximation of $\rvar{X}$ that is $\calB$-measurable precise, as long as we interpret \enquote{best} to mean closest in the $L^2$-norm.


\subsection{Independence}

Recall that we in \cref{sec:independence} defined what it means for a collection of $\sigma$-algebras to be independent. We now extend this definition to random variables.

\begin{definition}[Independence III]
    Let $(\Omega,\calF,\P)$ be a probability space. If $(\calC_i)_{i \in I}$ is a family of sets $\calC_i$ of events and $(\rvar{X}_j)_{j \in J}$ is a collection of random variables on $(\Omega,\calF,\P)$, then we say that the collection
    %
    \begin{equation*}
        \set{\calC_i}{i \in I}
            \union \set{\rvar{X}_j}{j \in J}
    \end{equation*}
    %
    is \keyword{independent} if the corresponding collection
    %
    \begin{equation*}
        \set{\calC_i}{i \in I}
            \union \set{\sigma(\rvar{X}_j)}{j \in J}
    \end{equation*}
    %
    is independent in the sense of \cref{def:independence-2}.
\end{definition}
%
In particular, if $\rvar{X}$ is a random variable and $\calB$ a sub-$\sigma$-algebra of $\calF$, then $\{ \rvar{X}, \calB \}$ is independent if
%
\begin{equation*}
    \P( \{\rvar{X} \in A\} \intersect B )
        = \P( \rvar{X} \in A ) \P(B)
\end{equation*}
%
for all $A \in \borel{\reals}$ and $B \in \calB$. The interpretation of this independence is quite natural: We need the information in $\sigma(\rvar{X})$ to determine the value $\rvar{X}$ assumes given some outcome. Say that this information is unknown to us. Then the best approximation of $\rvar{X}$ that we can make is just $\mean{\rvar{X}}$. Does knowing the information in the $\sigma$-algebra $\calB$ help us better approximate $\rvar{X}$? If $\sigma(\rvar{X})$ and $\calB$ are independent, then knowing whether the events in $\calB$ have occurred does not change our knowledge the events in $\sigma(\rvar{X})$, and hence does not enable us to make a better approximation. Hence we have have the following result:

\begin{proposition}
    Let $\rvar{X} \in \calL^1(\P)$ be a random variable on a probability space $(\Omega,\calF,\P)$, and let $\calB$ be a sub-$\sigma$-algebra of $\calF$. If $\rvar{X}$ and $\calB$ are independent, then
    %
    \begin{equation*}
        \condexp{\rvar{X}}{\calB}
            = \mean{\rvar{X}}.
    \end{equation*}
\end{proposition}

\begin{proof}
    Since $\rvar{X}$ and $\calB$ are independent, so are $\rvar{X}$ and $\indicator{B}$ for all $B \in \calB$. It follows that
    %
    \begin{equation*}
        \int_B \mean{\rvar{X}} \dif\P
            = \mean{\rvar{X}} \P(B)
            = \mean{\rvar{X}} \mean{\indicator{B}}
            = \mean{\rvar{X} \indicator{B}}
            = \int_B \rvar{X} \dif\P,
    \end{equation*}
    %
    proving the claim.
\end{proof}
%
This result in particular implies \cref{enum:conditional-expectation-no-information}, since $\calB = \{\emptyset, \Omega\}$ is independent of any random variable.



\nocite{*}

\printbibliography


\end{document}